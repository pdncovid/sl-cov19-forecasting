{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aEnJmuUfSNiw",
    "outputId": "bc8fadcf-3878-4f80-f655-4f7a35272e5a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import sys\n",
    "import os\n",
    "path = \"/content/drive/Shareddrives/covid.eng.pdn.ac.lk drive/COVID-AI (PG)/spatio_temporal/Covid19_DL_Forecasting_Codes\"\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Fz2cskQXii2"
   },
   "outputs": [],
   "source": [
    "!pip install pmdarima\n",
    "!pip install shap\n",
    "!pip install gluonts mxnet bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h2YSgdrxSMj0",
    "outputId": "bcd3cf70-b70c-41d1-a932-fb1fa692abb4"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "sys.path.insert(0, os.path.join(sys.path[0], '..'))\n",
    "import pandas as pd  # Basic library for all of our dataset operations\n",
    "import numpy as np\n",
    "import requests\n",
    "import io\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels as sm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import pmdarima as pm\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "\n",
    "import lightgbm as lgb\n",
    "# import gluonts\n",
    "from math import sqrt\n",
    "\n",
    "# data manipulation and signal processing\n",
    "import math\n",
    "import scipy\n",
    "from scipy import signal\n",
    "import scipy.stats as ss\n",
    "\n",
    "# plots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import folium\n",
    "\n",
    "# path = \"/con\n",
    "\n",
    "import shap\n",
    "warnings.filterwarnings(\"ignore\") #We will use deprecated models of statmodels which throw a lot of warnings to use more modern ones\n",
    "\n",
    "from utils.metrics import evaluate\n",
    "from utils.plots import bar_metrics, plot_prediction\n",
    "from utils.functions import split_into_pieces_inorder,split_into_pieces_random,create_dataset_random, distance, convert_lon_lat_to_adjacency_matrix\n",
    "from utils.data_loader import load_data, per_million, get_daily\n",
    "from utils.smoothing_functions import O_LPF,NO_LPF,O_NDA,NO_NDA\n",
    "\n",
    "from statsmodels.tsa.ar_model import AR\n",
    "from random import random\n",
    "from datetime import datetime\n",
    "# from fbprophet import Prophet\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing, ExponentialSmoothing\n",
    "from sklearn import linear_model, svm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from math import sqrt\n",
    "from xgboost import plot_importance, plot_tree\n",
    "from gluonts.model.deepar import DeepAREstimator\n",
    "from gluonts.trainer import Trainer\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from itertools import islice\n",
    "from pylab import rcParams\n",
    "# progress bar\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "\n",
    "#Extra settings\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "plt.style.use('bmh')\n",
    "mpl.rcParams['axes.labelsize'] = 14\n",
    "mpl.rcParams['xtick.labelsize'] = 12\n",
    "mpl.rcParams['ytick.labelsize'] = 12\n",
    "mpl.rcParams['text.color'] = 'k'\n",
    "mpl.rcParams['figure.figsize'] = 18, 8\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUFSyYY-SMj7",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Methods for time series forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcu_6oq_SMj7"
   },
   "source": [
    "There are many methods that we can use for time series forecasting and there is not a clear winner. Model selection should always depend on how you data look and what are you trying to achieve. Some models may be more robust against outliers but perform worse than the more sensible and could still be the best choice depending on the use case.\n",
    "\n",
    "When looking at your data the main split is wether we have extra regressors (features) to our time series or just the series. Based on this we can start exploring different methods for forecasting and their performance in different metrics.\n",
    "\n",
    "In this section we will show models for both cases, time series with and without extra regressors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StwMeREdSMj7"
   },
   "source": [
    "**Prepare data before modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQeDCCqYSMj8"
   },
   "source": [
    "Required variables:\n",
    "\n",
    "*   **region_names** - Names of the unique regions.\n",
    "*   **confirmed_cases** - 2D array. Each row should corresponds to values in 'region_names'. Each column represents a day. Columns should be in ascending order. (Starting day -> Present)\n",
    "*   **daily_cases** - confirmed_cases.diff()\n",
    "*   **population** - Population in 'region'\n",
    "*   **features** - Features of the regions. Each column is a certain feature.\n",
    "*   **START_DATE** - Starting date of the data DD/MM/YYYY\n",
    "*   **n_regions** Number of regions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tr_r9VIUSMj8"
   },
   "outputs": [],
   "source": [
    "daily_data = True\n",
    "DATASET = \"Sri Lanka\" #\"Sri Lanka\" \"Texas\" \"USA\" \"Global\"\n",
    "split_date = '2021-02-01'\n",
    "\n",
    "d = load_data(DATASET,path=\"../Datasets\")\n",
    "region_names=d[\"region_names\"] \n",
    "confirmed_cases=d[\"confirmed_cases\"] \n",
    "daily_cases=d[\"daily_cases\"] \n",
    "features=d[\"features\"] \n",
    "START_DATE=d[\"START_DATE\"] \n",
    "n_regions=d[\"n_regions\"] \n",
    "daily_cases[daily_cases<0] = 0\n",
    "population = features[\"Population\"]\n",
    "for i in range(len(population)):\n",
    "    print(\"{:.2f}%\".format(confirmed_cases[i,:].max()/population[i]*100), region_names[i])\n",
    "\n",
    "days = confirmed_cases.shape[1]\n",
    "\n",
    "print(f\"Total population {population.sum()/1e6:.2f}M, regions:{n_regions}, days:{days}\")\n",
    "\n",
    "daily_filtered = O_LPF(daily_cases, datatype='daily', order=3, R_weight=1.0, EIG_weight=1, corr = True, region_names=region_names)\n",
    "\n",
    "daily_per_mio_capita = per_million(daily_cases,population)\n",
    "daily_per_mio_capita_filtered = per_million(daily_filtered,population)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(daily_cases.T,columns = features.index)\n",
    "df.index = pd.to_datetime(pd.to_datetime(START_DATE).value + df.index*24*3600*1000000000)\n",
    "\n",
    "\n",
    "df_training = df.loc[df.index <= split_date]\n",
    "df_test = df.loc[df.index > split_date]\n",
    "print(f\"{len(df_training)} days of training data \\n {len(df_test)} days of testing data \")\n",
    "\n",
    "df_training.to_csv('../Datasets/training.csv')\n",
    "df_test.to_csv('../Datasets/test.csv')\n",
    "\n",
    "df_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3huAt018SMj8"
   },
   "outputs": [],
   "source": [
    "to_plot = ['KAL', 'GAL', 'GAM', 'HAM', 'JAF','KAN','MTL', 'MTR','TRI']\n",
    "# to_plot = [1,2,3,20]\n",
    "# to_plot =features.index\n",
    "\n",
    "plt.figure(figsize=(15, len(to_plot)))\n",
    "for i,tp in enumerate(to_plot):\n",
    "    plt.subplot(1+len(to_plot)//3,3,i+1)\n",
    "    plt.plot(df_training[tp], label=str(tp))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yQ2E1oISMj-"
   },
   "source": [
    "It is also very important to include some naive forecast as the series mean or previous value to make sure our models perform better than the simplest of the simplest. We dont want to introduce any complexity if it does not provides any performance gain. (*harshana I dont understand this statement fully*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6QoVVAlSMj-"
   },
   "outputs": [],
   "source": [
    "resultsDict={}\n",
    "predictionsDict={}\n",
    "gtDict = {}\n",
    "\n",
    "#Also add the naive mean average value\n",
    "mean = df_test.mean()\n",
    "mean = np.array([mean for _ in range(len(df_test))])\n",
    "resultsDict['Naive mean'] = evaluate(df_test.values, mean)\n",
    "predictionsDict['Naive mean'] = mean\n",
    "gtDict['Naive mean'] = df_test.values\n",
    "\n",
    "resultsDict['Yesterdays value'] = evaluate(df_test.values[:-1,:], df_test.values[1:,:])\n",
    "predictionsDict['Yesterdays value'] = df_test.values[:-1,:]\n",
    "gtDict['Yesterdays value'] = df_test.values[1:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8EQyYY9SMj-",
    "toc-hr-collapsed": false
   },
   "source": [
    "## Univariate-time-series-forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GT6RPTfSMj-"
   },
   "source": [
    "In this section we will focus on time series forecasting methods capable of only looking at the target variable. This means no other regressors (more variables) can be added into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUDzGC1vSMj_"
   },
   "source": [
    "### Simple Exponential Smoothing (SES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3iaVlZf9SMj_"
   },
   "source": [
    "The Simple Exponential Smoothing (SES) method models the next time step as an exponentially weighted linear function of observations at prior time steps. This method expects our time series to be non stationary in order to perform adecuately (no trend or seasonality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hqH-4w1bSMj_",
    "outputId": "d36ebced-fee3-4c47-f4d8-23c7e33fc38c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Walk throught the test data, training and predicting 1 day ahead for all the test data\n",
    "index = len(df_training)\n",
    "yhat = [[]]*n_regions\n",
    "for t in tqdm(range(len(df_test))):\n",
    "    temp_train = df.iloc[:len(df_training)+t,:]\n",
    "    for col in range(n_regions):\n",
    "        model = SimpleExpSmoothing(temp_train.iloc[:,col])\n",
    "        model_fit = model.fit()\n",
    "        predictions = model_fit.predict(start=len(temp_train), end=len(temp_train))\n",
    "        yhat[col] = yhat[col] + [predictions]\n",
    "    \n",
    "yhat = np.squeeze(np.array(yhat)).T\n",
    "resultsDict['SES'] = evaluate(df_test.values, yhat)\n",
    "predictionsDict['SES'] = yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgTcxmY1SMj_"
   },
   "source": [
    "### Holt Winter’s Exponential Smoothing (HWES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqCFJd9mSMkA"
   },
   "source": [
    "[HWES](https://machinelearningmastery.com/how-to-grid-search-triple-exponential-smoothing-for-time-series-forecasting-in-python/) or also known as triple exponential smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nf826mdqSMkA",
    "outputId": "289925c4-18e3-4a11-d6df-443e62a5aecf"
   },
   "outputs": [],
   "source": [
    "# Walk throught the test data, training and predicting 1 day ahead for all the test data\n",
    "index = len(df_training)\n",
    "yhat = [[]]*df_training.shape[1]\n",
    "for t in tqdm(range(len(df_test))):\n",
    "    temp_train = df.iloc[:len(df_training)+t,:]\n",
    "    for col in range(n_regions):\n",
    "        model = ExponentialSmoothing(temp_train.iloc[:,col])\n",
    "        model_fit = model.fit()\n",
    "        predictions = model_fit.predict(start=len(temp_train), end=len(temp_train))\n",
    "        yhat[col] = yhat[col] + [predictions]\n",
    "    \n",
    "yhat = np.squeeze(np.array(yhat)).T\n",
    "resultsDict['HWES'] = evaluate(df_test.values, yhat)\n",
    "predictionsDict['HWES'] = yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93XlgV2iSMkA"
   },
   "source": [
    "### Autoregression (AR)\n",
    "The autoregression (AR) method models the next step in the sequence as a linear function of the observations at prior time steps. Parameters of the model:\n",
    "\n",
    "- __Number of AR (Auto-Regressive) terms (p):__ p is the parameter associated with the auto-regressive aspect of the model, which incorporates past values i.e lags of dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1)….x(t-5).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8mme-p_lSMkA",
    "outputId": "e2eee062-a8b1-4811-c3af-adeadf91f701"
   },
   "outputs": [],
   "source": [
    "# Walk throught the test data, training and predicting 1 day ahead for all the test data\n",
    "index = len(df_training)\n",
    "yhat = [[]]*df_training.shape[1]\n",
    "for t in tqdm(range(len(df_test))):\n",
    "    temp_train = df.iloc[:len(df_training)+t,:]\n",
    "    for col in range(n_regions):\n",
    "        model = AR(temp_train.iloc[:,col].values)\n",
    "        model_fit = model.fit()\n",
    "        predictions = model_fit.predict(start=len(temp_train), end=len(temp_train))\n",
    "        yhat[col] = yhat[col] + [predictions]\n",
    "    \n",
    "yhat = np.squeeze(np.array(yhat)).T\n",
    "resultsDict['AR'] = evaluate(df_test.values, yhat)\n",
    "predictionsDict['AR'] = yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "1xyz_QZNSMkB",
    "outputId": "cf2fa5bc-79cd-4140-b968-11352dd50250"
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(15, len(to_plot)))\n",
    "for i,tp in enumerate(to_plot):\n",
    "    plt.subplot(1+len(to_plot)//3,3,i+1)\n",
    "    plt.plot(df_test[tp].values, label='Original '+str(tp))\n",
    "    plt.plot(yhat[:,list(df_test.columns).index(tp)],color='red',label='AR predicted '+str(tp))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDhhqxQ2YaKZ"
   },
   "source": [
    "*we can observe a little delay.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20lsAGAASMkB"
   },
   "source": [
    "### Moving Average (MA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VO8QGP4xSMkB"
   },
   "source": [
    "The Moving Average (MA) method models the next step in the sequence as the average of a window of observations at prior time steps. Parameters of the model:\n",
    "\n",
    "\n",
    "- __Number of MA (Moving Average) terms (q):__ q is size of the moving average part window of the model i.e. lagged forecast errors in prediction equation. For instance if q is 5, the predictors for x(t) will be e(t-1)….e(t-5) where e(i) is the difference between the moving average at ith instant and actual value. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IRTHyjHmSMkB",
    "outputId": "73523556-faad-4ee2-942f-131ca51ef894"
   },
   "outputs": [],
   "source": [
    "# MA example\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "from random import random\n",
    "\n",
    "# Walk throught the test data, training and predicting 1 day ahead for all the test data\n",
    "index = len(df_training)\n",
    "yhat = [[]]*df_training.shape[1]\n",
    "for t in tqdm(range(len(df_test))):\n",
    "    temp_train = df.iloc[:len(df_training)+t,:]\n",
    "    for col in range(n_regions):\n",
    "        model = ARMA(temp_train.iloc[:,col], order=(0, 1))\n",
    "        model_fit = model.fit(disp=False)\n",
    "        predictions = model_fit.predict(start=len(temp_train), end=len(temp_train))\n",
    "        yhat[col] = yhat[col] + [predictions]\n",
    "    \n",
    "yhat = np.squeeze(np.array(yhat)).T\n",
    "resultsDict['MA'] = evaluate(df_test.values, yhat)\n",
    "predictionsDict['MA'] = yhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "kFlrfEp3SMkB",
    "outputId": "bb794349-dfe1-416c-966e-6c246cda278f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, len(to_plot)))\n",
    "for i,tp in enumerate(to_plot):\n",
    "    plt.subplot(1+len(to_plot)//3,3,i+1)\n",
    "    plt.plot(df_test[tp].values, label='Original '+str(tp))\n",
    "    plt.plot(yhat[:,list(df_test.columns).index(tp)],color='red',label='MA predicted '+str(tp))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIw7YvdFYoFQ"
   },
   "source": [
    "*this is also not fitting ne?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsCiivzZSMkC"
   },
   "source": [
    "### Autoregressive Moving Average (ARMA) - Not fitting!\n",
    "\n",
    "This method will basically join the previous two `AR` and `MA`. Model parameters will be the sum of the two.\n",
    "\n",
    "- __Number of AR (Auto-Regressive) terms (p):__ p is the parameter associated with the auto-regressive aspect of the model, which incorporates past values i.e lags of dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1)….x(t-5).\n",
    "- __Number of MA (Moving Average) terms (q):__ q is size of the moving average part window of the model i.e. lagged forecast errors in prediction equation. For instance if q is 5, the predictors for x(t) will be e(t-1)….e(t-5) where e(i) is the difference between the moving average at ith instant and actual value. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vztN2bExSMkC"
   },
   "outputs": [],
   "source": [
    "# # ARMA example\n",
    "# from statsmodels.tsa.arima_model import ARMA\n",
    "# from random import random\n",
    "\n",
    "# # Walk throught the test data, training and predicting 1 day ahead for all the test data\n",
    "# index = len(df_training)\n",
    "# yhat = [[]]*df_training.shape[1]\n",
    "# for t in tqdm(range(len(df_test))):\n",
    "#     temp_train = df.iloc[:len(df_training)+t,:]\n",
    "#     for col in range(n_regions):\n",
    "#         model = ARMA(temp_train.iloc[:,col], order=(1, 1))\n",
    "        \n",
    "#         model_fit = model.fit(disp=True)\n",
    "#         predictions = model_fit.predict(start=len(temp_train), end=len(temp_train), dynamic=False)\n",
    "#         yhat[col] = yhat[col] + [predictions]\n",
    "    \n",
    "# yhat = np.squeeze(np.array(yhat)).T\n",
    "# resultsDict['ARMA'] = evaluate(df_test.values, yhat)\n",
    "# predictionsDict['ARMA'] = yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZRSmlb3qSMkC"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15, len(to_plot)))\n",
    "# for i,tp in enumerate(to_plot):\n",
    "#     plt.subplot(1+len(to_plot)//3,3,i+1)\n",
    "#     plt.plot(df_test[tp].values, label='Original '+tp)\n",
    "#     plt.plot(yhat[:,list(df_test.columns).index(tp)],color='red',label='ARMA predicted '+tp)\n",
    "#     plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlgpwL74SMkC",
    "toc-hr-collapsed": true
   },
   "source": [
    "### Autoregressive integrated moving average (ARIMA) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uB3kw0YkSMkC"
   },
   "source": [
    "In an ARIMA model there are 3 parameters that are used to help model the major aspects of a times series: seasonality, trend, and noise. These parameters are labeled p,d,and q.\n",
    "\n",
    "* Number of AR (Auto-Regressive) terms (p): p is the parameter associated with the auto-regressive aspect of the model, which incorporates past values i.e lags of dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1)….x(t-5).\n",
    "* Number of Differences (d): d is the parameter associated with the integrated part of the model, which effects the amount of differencing to apply to a time series.\n",
    "* Number of MA (Moving Average) terms (q): q is size of the moving average part window of the model i.e. lagged forecast errors in prediction equation. For instance if q is 5, the predictors for x(t) will be e(t-1)….e(t-5) where e(i) is the difference between the moving average at ith instant and actual value.\n",
    "\n",
    "**Tuning ARIMA parameters**\n",
    "\n",
    "Non stationarity series will require level of differencing (d) >0 in ARIMA\n",
    "Select the lag values for the Autoregression (AR) and Moving Average (MA) parameters, p and q respectively, using PACF, ACF plots\n",
    "AUTOARIMA\n",
    "\n",
    "Note: A problem with ARIMA is that it does not support seasonal data. That is a time series with a repeating cycle. ARIMA expects data that is either not seasonal or has the seasonal component removed, e.g. seasonally adjusted via methods such as seasonal differencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3QaM2i5OSMkD",
    "outputId": "f3ce58d5-8ed9-4311-a5bd-b8a9cab82838"
   },
   "outputs": [],
   "source": [
    "# ARIMA example\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Walk throught the test data, training and predicting 1 day ahead for all the test data\n",
    "index = len(df_training)\n",
    "yhat = [[]]*df_training.shape[1]\n",
    "for t in tqdm(range(len(df_test))):\n",
    "    temp_train = df.iloc[:len(df_training)+t,:]\n",
    "    for col in range(n_regions):\n",
    "        model = ARIMA(temp_train.iloc[:,col], order=(1,0, 0))\n",
    "        model_fit = model.fit(disp=False)\n",
    "        predictions = model_fit.predict(start=len(temp_train), end=len(temp_train), dynamic=False)\n",
    "        yhat[col] = yhat[col] + [predictions]\n",
    "    \n",
    "yhat = np.squeeze(np.array(yhat)).T\n",
    "resultsDict['ARIMA'] = evaluate(df_test.values, yhat)\n",
    "predictionsDict['ARIMA'] = yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "n_BkxkFISMkD",
    "outputId": "af14a838-2bf4-4851-e655-66445b4f39e0"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, len(to_plot)))\n",
    "for i,tp in enumerate(to_plot):\n",
    "    plt.subplot(1+len(to_plot)//3,3,i+1)\n",
    "    plt.plot(df_test[tp].values, label='Original '+tp)\n",
    "    plt.plot(yhat[:,list(df_test.columns).index(tp)],color='red',label='ARMA predicted '+tp)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CR21Ilh6SMkD"
   },
   "source": [
    "#### Auto ARIMA - Not fitting for some"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZ8aKJ9LSMkD",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #building the model\n",
    "# autoModels = []\n",
    "# for col in range(df_training.shape[1]):\n",
    "#     autoModel = pm.auto_arima(df.iloc[:,col], trace=True, error_action='ignore', suppress_warnings=True,seasonal=False)\n",
    "#     autoModel.fit(df.iloc[:,col])\n",
    "#     autoModels.append(autoModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yz1ZaEn7SMkE"
   },
   "outputs": [],
   "source": [
    "\n",
    "# yhat = [[]]*df_training.shape[1]\n",
    "# for t in tqdm(range(len(df_test))):\n",
    "#     temp_train = df.iloc[:len(df_training)+t,:]\n",
    "#     for col in range(n_regions):\n",
    "#         order = autoModels[col].order\n",
    "#         print(col)\n",
    "#         model = ARIMA(temp_train.iloc[:,col], order=order)\n",
    "#         model_fit = model.fit(disp=False)\n",
    "#         predictions = model_fit.predict(start=len(temp_train), end=len(temp_train), dynamic=False)\n",
    "#         yhat[col] = yhat[col] + [predictions]\n",
    "    \n",
    "# yhat = np.squeeze(np.array(yhat)).T\n",
    "# resultsDict['AutoARIMA'] = evaluate(df_test.values, yhat)\n",
    "# predictionsDict['AutoARIMA'] = yhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0FXa40aNSMkE"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15, len(to_plot)))\n",
    "# for i,tp in enumerate(to_plot):\n",
    "#     plt.subplot(1+len(to_plot)//3,3,i+1)\n",
    "#     plt.plot(df_test[tp].values, label='Original '+tp)\n",
    "#     plt.plot(yhat[:,list(df_test.columns).index(tp)],color='red',label='AutoARMA '+tp)\n",
    "#     plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipoOGE5gSMkE",
    "toc-hr-collapsed": true
   },
   "source": [
    "### Seasonal Autoregressive Integrated Moving-Average (SARIMA)\n",
    "Seasonal Autoregressive Integrated Moving Average, SARIMA or Seasonal ARIMA, is an extension of ARIMA that explicitly supports univariate time series data with a seasonal component.\n",
    "\n",
    "It adds three new hyperparameters to specify the autoregression (AR), differencing (I) and moving average (MA) for the seasonal component of the series, as well as an additional parameter for the period of the seasonality.\n",
    "\n",
    "__Trend Elements:__\n",
    "\n",
    "There are three trend elements that require configuration. They are the same as the ARIMA model, specifically:\n",
    "\n",
    "- p: Trend autoregression order.\n",
    "- d: Trend difference order.\n",
    "- q: Trend moving average order.\n",
    "\n",
    "__Seasonal Elements:__\n",
    "\n",
    "There are four seasonal elements that are not part of ARIMA that must be configured; they are:\n",
    "\n",
    "- P: Seasonal autoregressive order.\n",
    "- D: Seasonal difference order.\n",
    "- Q: Seasonal moving average order.\n",
    "- m: The number of time steps for a single seasonal period. For example, an S of 12 for monthly data suggests a yearly seasonal cycle.\n",
    "\n",
    "__SARIMA notation:__\n",
    "SARIMA(p,d,q)(P,D,Q,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x7_PIJ3kSMkE",
    "outputId": "cb434f14-aa94-4105-ac82-8b5338fc34da"
   },
   "outputs": [],
   "source": [
    "# SARIMA example\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Walk throught the test data, training and predicting 1 day ahead for all the test data\n",
    "yhat = [[]]*df_training.shape[1]\n",
    "for t in tqdm(range(len(df_test))):\n",
    "    temp_train = df.iloc[:len(df_training)+t,:]\n",
    "    for col in range(n_regions):\n",
    "        model = SARIMAX(temp_train.iloc[:,col], order=(1,0, 0), seasonal_order=(0, 0, 0, 3))\n",
    "        model_fit = model.fit(disp=False)\n",
    "        predictions = model_fit.predict(start=len(temp_train), end=len(temp_train), dynamic=False)\n",
    "        yhat[col] = yhat[col] + [predictions]\n",
    "    \n",
    "yhat = np.squeeze(np.array(yhat)).T\n",
    "resultsDict['SARIMAX'] = evaluate(df_test.values, yhat)\n",
    "predictionsDict['SARIMAX'] = yhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "NT6bSajiSMkE",
    "outputId": "f78ddc5e-e219-492e-fade-519c975955a2"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, len(to_plot)))\n",
    "for i,tp in enumerate(to_plot):\n",
    "    plt.subplot(1+len(to_plot)//3,3,i+1)\n",
    "    plt.plot(df_test[tp].values, label='Original '+tp)\n",
    "    plt.plot(yhat[:,list(df_test.columns).index(tp)],color='red',label='SARIMAX '+tp)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9-3vqhKY_QB"
   },
   "source": [
    "*seems to fit that the predicted signal is only a delay from the original signal*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJ3mtfZuSMkF"
   },
   "source": [
    "#### Auto - SARIMA\n",
    "\n",
    "[auto_arima documentation for selecting best model](https://www.alkaline-ml.com/pmdarima/tips_and_tricks.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KoU0u8ipSMkF",
    "outputId": "b56c1613-caab-4262-a693-1b5b5750d97e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#building the model\n",
    "autoModels = []\n",
    "for col in range(df_training.shape[1]):\n",
    "    print(col)\n",
    "    autoModel = pm.auto_arima(df_training.iloc[:,col], trace=True, error_action='ignore', suppress_warnings=True,seasonal=True, m=6, stepwise=True)\n",
    "    autoModel.fit(df_training.iloc[:,col])\n",
    "    autoModels.append(autoModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OjbFuBoQSMkF",
    "outputId": "aa13d421-e361-41f0-b89e-9cf1d5134e37",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "yhat = [[]]*df_training.shape[1]\n",
    "for t in tqdm(range(len(df_test))):\n",
    "    temp_train = df.iloc[:len(df_training)+t,:]\n",
    "    for col in range(n_regions):\n",
    "        order = autoModels[col].order\n",
    "        seasonalOrder = autoModels[col].seasonal_order\n",
    "        model = SARIMAX(temp_train.iloc[:,col],  order=order, seasonal_order=seasonalOrder)\n",
    "        model_fit = model.fit(disp=False)\n",
    "        predictions = model_fit.predict(start=len(temp_train), end=len(temp_train), dynamic=False)\n",
    "        yhat[col] = yhat[col] + [predictions]\n",
    "    \n",
    "yhat = np.squeeze(np.array(yhat)).T\n",
    "resultsDict['AutoSARIMAX'] = evaluate(df_test.values, yhat)\n",
    "predictionsDict['AutoSARIMAX'] = yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "fTWxsOxSSMkF",
    "outputId": "35a76884-a2fd-4a9f-b912-c7fab2ca53aa"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, len(to_plot)))\n",
    "for i,tp in enumerate(to_plot):\n",
    "    plt.subplot(1+len(to_plot)//3,3,i+1)\n",
    "    plt.plot(df_test[tp].values, label='Original '+tp)\n",
    "    plt.plot(yhat[:,list(df_test.columns).index(tp)],color='red',label='AutoSARIMAX '+tp)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GwAWbuiSMkF"
   },
   "source": [
    "### Prophet - Cannot install\n",
    "\n",
    "Prophet is a model released by [facebook](https://github.com/facebook/prophet). Is essentially a curve fitting approach, very similar in spirit to how BSTS models trend and seasonality, except that it uses generalized additive models instead of a state-space representation to describe each component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hq1HtyilSMkG"
   },
   "outputs": [],
   "source": [
    "# #Prophet needs some specifics data stuff, coment it here\n",
    "# prophet_training = df_training.rename(columns={'pollution_today': 'y'})  # old method  \n",
    "# prophet_training['ds'] = prophet_training.index\n",
    "# prophet_training.index = pd.RangeIndex(len(prophet_training.index))\n",
    "\n",
    "# prophet_test = df_test.rename(columns={'pollution_today': 'y'})  # old method  \n",
    "# prophet_test['ds'] = prophet_test.index\n",
    "# prophet_test.index = pd.RangeIndex(len(prophet_test.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FrUNH5yLSMkG"
   },
   "outputs": [],
   "source": [
    "# prophet = Prophet(\n",
    "#     growth='linear', \n",
    "#     seasonality_mode='multiplicative',\n",
    "#     holidays_prior_scale=20, \n",
    "#     daily_seasonality=False, \n",
    "#     weekly_seasonality=False, \n",
    "#     yearly_seasonality=False\n",
    "#     ).add_seasonality(\n",
    "#         name='monthly',\n",
    "#         period=30.5,\n",
    "#         fourier_order=55\n",
    "#     ).add_seasonality(\n",
    "#         name='daily',\n",
    "#         period=1,\n",
    "#         fourier_order=15\n",
    "#     ).add_seasonality(\n",
    "#         name='weekly',\n",
    "#         period=7,\n",
    "#         fourier_order=25\n",
    "#     ).add_seasonality(\n",
    "#         name='yearly',\n",
    "#         period=365.25,\n",
    "#         fourier_order=20\n",
    "#     ).add_seasonality(\n",
    "#         name='quarterly',\n",
    "#         period=365.25/4,\n",
    "#         fourier_order=55\n",
    "#     ).add_country_holidays(country_name='China')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QU_VX2r9SMkG"
   },
   "outputs": [],
   "source": [
    "# prophet.fit(prophet_training)\n",
    "# yhat = prophet.predict(prophet_test)\n",
    "# resultsDict['Prophet univariate'] = evaluate(df_test.pollution_today, yhat.yhat.values)\n",
    "# predictionsDict['Prophet univariate'] = yhat.yhat.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTUzppuBSMkG"
   },
   "outputs": [],
   "source": [
    "# plt.plot(df_test.pollution_today.values , label='Original')\n",
    "# plt.plot(yhat.yhat,color='red',label='Prophet univariate')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve4xwbG4SMkG",
    "toc-hr-collapsed": false
   },
   "source": [
    "## Multivariate time series forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GuiCZu-nSMkG"
   },
   "outputs": [],
   "source": [
    "# ## ADD time features to our model\n",
    "# def create_time_features(df,target=None):\n",
    "#     \"\"\"\n",
    "#     Creates time series features from datetime index\n",
    "#     \"\"\"\n",
    "#     df['date'] = df.index\n",
    "#     df['hour'] = df['date'].dt.hour\n",
    "#     df['dayofweek'] = df['date'].dt.dayofweek\n",
    "#     df['quarter'] = df['date'].dt.quarter\n",
    "#     df['month'] = df['date'].dt.month\n",
    "#     df['year'] = df['date'].dt.year\n",
    "#     df['dayofyear'] = df['date'].dt.dayofyear\n",
    "#     df['sin_day'] = np.sin(df['dayofyear'])\n",
    "#     df['cos_day'] = np.cos(df['dayofyear'])\n",
    "#     df['dayofmonth'] = df['date'].dt.day\n",
    "#     df['weekofyear'] = df['date'].dt.weekofyear\n",
    "#     X = df.drop(['date'],axis=1)\n",
    "#     if target:\n",
    "#         y = df[target]\n",
    "#         X = X.drop([target],axis=1)\n",
    "#         return X, y\n",
    "    \n",
    "#     return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umumTAckSMkG"
   },
   "outputs": [],
   "source": [
    "# X_train_df, y_train = create_time_features(df_training, target='pollution_today')\n",
    "# X_test_df, y_test = create_time_features(df_test, target='pollution_today')\n",
    "# scaler = StandardScaler() \n",
    "# scaler.fit(X_train_df) #No cheating, never scale on the training+test!\n",
    "# X_train = scaler.transform(X_train_df)  \n",
    "# X_test = scaler.transform(X_test_df)\n",
    "\n",
    "# X_train_df = pd.DataFrame(X_train,columns=X_train_df.columns)\n",
    "# X_test_df = pd.DataFrame(X_test,columns=X_test_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5b30-vySMkH"
   },
   "source": [
    "Required\n",
    "\n",
    "X = (Days, features)\n",
    "Y = (Days,)\n",
    "\n",
    "But we don't have time series features for each districts. Therefore number of features is 1 (covid cases).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OQ5QEzTSMkH",
    "toc-hr-collapsed": false
   },
   "source": [
    "### Linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWifPcYLSMkH"
   },
   "source": [
    "#### Bayesian regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HyuP4h8tSMkH"
   },
   "outputs": [],
   "source": [
    "today = df_training.iloc[:-1,col:col+1]\n",
    "tomorrow = df_training.iloc[1:,col:col+1]\n",
    "yhat = []\n",
    "for col in range(n_regions):\n",
    "    reg = linear_model.BayesianRidge()\n",
    "\n",
    "    reg.fit(today, tomorrow)\n",
    "    yhat.append(reg.predict(df_test.iloc[:,col:col+1]))\n",
    "    \n",
    "yhat = np.squeeze(np.array(yhat)).T\n",
    "resultsDict['BayesianRidge'] = evaluate(df_test.values, yhat)\n",
    "predictionsDict['BayesianRidge'] = yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbSEXc4VSMkH"
   },
   "source": [
    "#### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "De9SE4ibSMkH"
   },
   "outputs": [],
   "source": [
    "today = df_training.iloc[:-1,col:col+1]\n",
    "tomorrow = df_training.iloc[1:,col:col+1]\n",
    "yhat = []\n",
    "for col in range(n_regions):\n",
    "    reg = linear_model.Lasso(alpha=0.1)\n",
    "    reg.fit(today, tomorrow)\n",
    "    yhat.append(reg.predict(df_test.iloc[:,col:col+1]))\n",
    "    \n",
    "yhat = np.squeeze(np.array(yhat)).T\n",
    "resultsDict['Lasso'] = evaluate(df_test.values, yhat)\n",
    "predictionsDict['Lasso'] = yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJbwJvwNSMkH",
    "toc-hr-collapsed": true
   },
   "source": [
    "### Tree models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLj27kBBSMkI"
   },
   "source": [
    "#### Randomforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WcwEH9Y-SMkI"
   },
   "outputs": [],
   "source": [
    "yhat = []\n",
    "for col in range(n_regions):\n",
    "    reg = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "    reg.fit(df_training.iloc[:-1,col:col+1], df_training.iloc[1:,col:col+1])\n",
    "    yhat.append(reg.predict(df_test.iloc[:,col:col+1]))\n",
    "    \n",
    "yhat = np.squeeze(np.array(yhat)).T\n",
    "resultsDict['Randomforest'] = evaluate(df_test.values, yhat)\n",
    "predictionsDict['Randomforest'] = yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pg6q5UwKSMkI",
    "toc-hr-collapsed": true
   },
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hlh--nEbSMkI"
   },
   "outputs": [],
   "source": [
    "yhat = []\n",
    "for col in range(n_regions):\n",
    "    reg = xgb.XGBRegressor(objective ='reg:squarederror',n_estimators=1000)\n",
    "    reg.fit(df_training.iloc[:-1,col:col+1], df_training.iloc[1:,col:col+1], verbose=False)\n",
    "    yhat.append(reg.predict(df_test.iloc[:,col:col+1]))\n",
    "    \n",
    "yhat = np.squeeze(np.array(yhat)).T\n",
    "resultsDict['XGBoost'] = evaluate(df_test.values, yhat)\n",
    "predictionsDict['XGBoost'] = yhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BwiUBjScSMkI",
    "outputId": "6e1c8d25-c8c1-4965-c990-22a0539ef8cf"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, len(to_plot)))\n",
    "for i,tp in enumerate(to_plot):\n",
    "    plt.subplot(1+len(to_plot)//3,3,i+1)\n",
    "    plt.plot(df_test[tp].values, label='Original '+tp)\n",
    "    plt.plot(yhat[:,list(df_test.columns).index(tp)],color='red',label='XGBoost '+tp)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NiU3GWOQSMkI"
   },
   "source": [
    "#### Lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSczxBpRSMkI"
   },
   "source": [
    "A tree gradient boosting model by [microsoft](https://github.com/microsoft/LightGBM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-kuivCvjSMkJ"
   },
   "outputs": [],
   "source": [
    "yhat = []\n",
    "for col in range(n_regions):\n",
    "    reg = lgb.LGBMRegressor()\n",
    "    reg.fit(df_training.iloc[:-1,col:col+1], df_training.iloc[1:,col:col+1])\n",
    "    yhat.append(reg.predict(df_test.iloc[:,col:col+1]))\n",
    "    \n",
    "yhat = np.squeeze(np.array(yhat)).T\n",
    "resultsDict['Lightgbm'] = evaluate(df_test.values, yhat)\n",
    "predictionsDict['Lightgbm'] = yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEHrkHKoSMkJ"
   },
   "source": [
    "### Support vector machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUuvUZBMSMkJ"
   },
   "source": [
    "Explain multiple kernels balbla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3znZRb9ASMkJ"
   },
   "outputs": [],
   "source": [
    "yhat = []\n",
    "for col in range(n_regions):\n",
    "    reg = svm.SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)\n",
    "    reg.fit(df_training.iloc[:-1,col:col+1], df_training.iloc[1:,col:col+1])\n",
    "    yhat.append(reg.predict(df_test.iloc[:,col:col+1]))\n",
    "    \n",
    "yhat = np.squeeze(np.array(yhat)).T\n",
    "resultsDict['SVM RBF'] = evaluate(df_test.values, yhat)\n",
    "predictionsDict['SVM RBF'] = yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KKwh6cjSMkJ"
   },
   "source": [
    "### Nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRcVZpmDSMkJ"
   },
   "outputs": [],
   "source": [
    "yhat = []\n",
    "for col in range(n_regions):\n",
    "    reg = KNeighborsRegressor(n_neighbors=2)\n",
    "    reg.fit(df_training.iloc[:-1,col:col+1], df_training.iloc[1:,col:col+1])\n",
    "    yhat.append(reg.predict(df_test.iloc[:,col:col+1]))\n",
    "    \n",
    "yhat = np.squeeze(np.array(yhat)).T\n",
    "resultsDict['Kneighbors'] = evaluate(df_test.values, yhat)\n",
    "predictionsDict['Kneighbors'] = yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBaC48IKSMkJ"
   },
   "source": [
    "### Prophet multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZsK2ld84SMkK"
   },
   "outputs": [],
   "source": [
    "# prophet = Prophet(\n",
    "#     growth='linear', \n",
    "#     seasonality_mode='multiplicative',\n",
    "#     daily_seasonality=True, \n",
    "#     ).add_country_holidays(country_name='China')\n",
    "\n",
    "\n",
    "# for col in prophet_training.columns:\n",
    "#     if col not in [\"ds\", \"y\"]:\n",
    "#         prophet.add_regressor(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3aT2UPscSMkK"
   },
   "outputs": [],
   "source": [
    "# prophet.fit(prophet_training)\n",
    "# yhat = prophet.predict(prophet_test)\n",
    "# resultsDict['Prophet multivariate'] = evaluate(y_test, yhat.yhat.values)\n",
    "# predictionsDict['Prophet multivariate'] = yhat.yhat.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZF0NtEhESMkK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.plot(df_test.pollution_today.values , label='Original')\n",
    "# plt.plot(yhat.yhat,color='red',label='Prophet multivariate')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "besQd6uESMkK",
    "toc-hr-collapsed": false
   },
   "source": [
    "### Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WFaOO6hSMkK"
   },
   "source": [
    "#### Tensorlfow LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2FKb7ZESMkK"
   },
   "source": [
    "LSTM are a special type of neural network architecture, you can read more on this [here](https://www.tensorflow.org/guide/keras/rnn)\n",
    "\n",
    "We will be trying a LSTM model for our benchmark but we will need to reshape our data to provide the network a window of previous samples (past days data) for each y target value. Find the code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load previous models and visualize prediction for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filtered, normalize=False):\n",
    "    if filtered == False:\n",
    "        x,y= np.copy(daily_cases), np.copy(daily_cases)\n",
    "    else:\n",
    "        x,y= np.copy(daily_filtered), np.copy(daily_filtered)\n",
    "    \n",
    "    \n",
    "    x = per_million(x, population)\n",
    "    y = per_million(y, population)\n",
    "    if normalize:\n",
    "        \n",
    "        x, xs=normalize_for_nn(x, None if type(normalize)==bool else normalize)\n",
    "        y, xs=normalize_for_nn(y, xs)\n",
    "        return x.T,y.T,xs\n",
    "    else:\n",
    "        return x.T,y.T\n",
    "    \n",
    "# ==================================== TODO: find a good normalization technique\n",
    "def normalize_for_nn(data, given_scalers=None):\n",
    "    data = np.copy(data)\n",
    "    print(f\"NORMALIZING; Data: {data.shape} expected (regions, days)\")\n",
    "#     data = np.log(data.astype('float32')+1)\n",
    "    scalers = []\n",
    "    scale = float(np.max(data[:,:]))\n",
    "    for i in range(data.shape[0]):\n",
    "        if given_scalers is not None:\n",
    "            scale = given_scalers[i]\n",
    "        else:\n",
    "            scale = float(np.max(data[i,:]))\n",
    "        scalers.append(scale)\n",
    "        data[i,:] /= scale\n",
    "#     print(\"NAN\",np.isnan(data).sum())\n",
    "    \n",
    "    return data, scalers\n",
    "\n",
    "def undo_normalization(normalized_data, scalers):\n",
    "    normalized_data = np.copy(normalized_data)\n",
    "    if len(normalized_data.shape) == 2:\n",
    "        normalized_data = np.expand_dims(normalized_data,0)\n",
    "    \n",
    "    print(f\"DENORMALIZING; Norm Data: {normalized_data.shape} expected (samples, windowsize, region)\")\n",
    "    for i in range(len(scalers)):\n",
    "        normalized_data[:,:,i] *= scalers[i]\n",
    "#     normalized_data[normalized_data>10] = np.nan\n",
    "#     normalized_data = np.exp(normalized_data)-1\n",
    "#     print(\"NAN\",np.isnan(normalized_data).sum())\n",
    "    return normalized_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = '2021-02-01'\n",
    "split_days = (pd.to_datetime(split_date)-pd.to_datetime(START_DATE)).days+2\n",
    "x_data, y_data, x_data_scalers = get_data(False,normalize=True)\n",
    "x_dataf, y_dataf, x_data_scalersf = get_data(True, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ipNvoFZxSMkM",
    "outputId": "74e43261-6211-433d-fddb-516807204068",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_model_predictions(model,x_data,y_data, scalers):\n",
    "    \n",
    "    WINDOW_LENGTH = model.input.shape[1] \n",
    "    PREDICT_STEPS = model.output.shape[1]\n",
    "    \n",
    "    print(f\"Predicting from model. X={x_data.shape} Y={y_data.shape}\")\n",
    "    # CREATING TRAIN-TEST SETS FOR CASES\n",
    "    x_test, y_test = split_into_pieces_inorder(x_data.T,y_data.T, WINDOW_LENGTH, PREDICT_STEPS, WINDOW_LENGTH+PREDICT_STEPS, reduce_last_dim=False)\n",
    "    print(\"Input data shape\", x_test.shape)\n",
    "    if len(model.output.shape)==2:\n",
    "        y_pred = np.zeros_like(y_test)\n",
    "        for i in range(len(region_names)):\n",
    "            y_pred[:,:,i] = model(x_test[:,:,i:i+1])\n",
    "    else:\n",
    "        y_pred = model(x_test).numpy()\n",
    "    print(\"Predicted shape\", y_pred.shape)\n",
    "    # # NOTE:\n",
    "    # # max value may change with time. then we have to retrain the model!!!!!!\n",
    "    # # we can have a predefined max value. 1 for major cities and 1 for smaller districts\n",
    "    x_test = undo_normalization(x_test, scalers)\n",
    "    y_test = undo_normalization(y_test, scalers)\n",
    "    y_pred = undo_normalization(y_pred, scalers)\n",
    "\n",
    "    return x_test, y_test, y_pred\n",
    "\n",
    "model =  tf.keras.models.load_model(\"models/Sri Lanka_LSTM_Filtered.h5\")\n",
    "x_data, y_data, _ = get_data(filtered=False, normalize=x_data_scalers)\n",
    "x_test, y_test, y_pred = get_model_predictions(model,x_data,y_data, x_data_scalers)\n",
    "x_data, y_data, _ = get_data(filtered=True, normalize=x_data_scalers)\n",
    "x_testf, y_testf, y_predf = get_model_predictions(model,x_data,y_data, x_data_scalers)\n",
    "\n",
    "model =  tf.keras.models.load_model(\"models/Sri Lanka_LSTM_Unfiltered.h5\")\n",
    "x_data, y_data, _ = get_data(filtered=False, normalize=x_data_scalers)\n",
    "x_test, y_test, y_pred_unfmodel = get_model_predictions(model,x_data,y_data,x_data_scalers)\n",
    "\n",
    "# model =  tf.keras.models.load_model(\"models/Texas_LSTM_Filtered.h5\")\n",
    "# x_data, y_data, _ = get_data(filtered=False, normalize=True)\n",
    "# X_test, Y_test, Y_pred_texmodel = get_model_predictions(model,x_data,y_data,x_data_scalers)\n",
    "\n",
    "Ys = np.stack([y_test, y_testf, y_pred,y_predf,y_pred_unfmodel], 1)\n",
    "method_list = ['Observations Raw',\n",
    "               'Observations Filtered',\n",
    "               'LSTM-F-SL (R-SL)',\n",
    "               'LSTM-F-SL (F-SL)', \n",
    "               'LSTM-R-SL (R-SL)',\n",
    "#                'Predictions using Raw data (Model trained on Filtered Texas data)',\n",
    "               ]\n",
    "styles = {\n",
    "    'X':{'Preprocessing':'Raw','Data':'Training', 'Size':2},\n",
    "    'Xf':{'Preprocessing':'Filtered','Data':'Training', 'Size':2},\n",
    "    'Observations Raw':{'Preprocessing':'Raw','Data':'Training', 'Size':2},\n",
    "    'Observations Filtered':{'Preprocessing':'Filtered','Data':'Training', 'Size':2},\n",
    "    'LSTM-F-SL (R-SL)':{'Preprocessing':'Raw','Data':'LSTM-F-SL (R-SL)', 'Size':4},\n",
    "    'LSTM-F-SL (F-SL)':{'Preprocessing':'Filtered','Data':'LSTM-F-SL (F-SL)', 'Size':3},\n",
    "    'LSTM-R-SL (R-SL)':{'Preprocessing':'Raw','Data':'LSTM-R-SL (R-SL)', 'Size':4},\n",
    "}\n",
    "x_data, y_data = get_data(filtered=False, normalize=False)\n",
    "# region_mask = (np.mean(x_data,0) > 140).astype('int32')\n",
    "region_mask = (np.arange(n_regions) == 4).astype('int32')\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_prediction(x_test,x_testf, Ys, method_list, styles, region_names, region_mask)\n",
    "\n",
    "plt.savefig(f\"images/{DATASET}.eps\")\n",
    "plt.savefig(f\"images/{DATASET}.jpg\")\n",
    "plt.show()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ohx4X1x65txr"
   },
   "source": [
    "### Continuous prediction into future from given sequence of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction of next day from last 14 days for the test period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "haNP5Yh7SMkM",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def window_data(X,Y,window=7):\n",
    "    '''\n",
    "    The dataset length will be reduced to guarante all samples have the window, so new length will be len(dataset)-window\n",
    "    '''\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for i in range(window-1,len(X)):\n",
    "        x.append(X[i-window+1:i+1]) \n",
    "        y.append(Y[i])\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "def get_model_predictions(model,x_data,y_data, scalers):\n",
    "    WINDOW_LENGTH = model.input.shape[1] \n",
    "    PREDICT_STEPS = model.output.shape[1]\n",
    "    print(f\"Predicting from model. X={x_data.shape} Y={y_data.shape}\")\n",
    "    X_w,y_w = window_data(x_data,y_data,window=WINDOW_LENGTH)\n",
    "    \n",
    "    X_test_w = X_w[split_days-WINDOW_LENGTH-1:-1]\n",
    "    y_test_w = y_w[split_days-WINDOW_LENGTH-1:-1]\n",
    "    \n",
    "    yhat = []\n",
    "    for col in range(n_regions):\n",
    "        yhat.append(model.predict(X_test_w[:,:,col:col+1])[:,0].reshape(1,-1)[0])\n",
    "\n",
    "    yhat = np.squeeze(np.array(yhat)).T\n",
    "    \n",
    "    yhat = undo_normalization(yhat, scalers)[0]\n",
    "    y_test_w = undo_normalization(y_test_w, scalers)[0]\n",
    "    return X_test_w, y_test_w, yhat\n",
    "\n",
    "#########################################################################\n",
    "\n",
    "model =  tf.keras.models.load_model(\"models/Sri Lanka_LSTM_Filtered.h5\")\n",
    "\n",
    "x_data, y_data, _ = get_data(filtered=False, normalize=x_data_scalers)\n",
    "_, y_test, yhat = get_model_predictions(model,x_data,y_data, x_data_scalers)\n",
    "resultsDict['LSTM-F-SL (Raw)'] = evaluate(y_test, yhat)\n",
    "predictionsDict['LSTM-F-SL (Raw)'] = yhat\n",
    "gtDict['LSTM-F-SL (Raw)'] = y_test\n",
    "\n",
    "x_dataf, y_dataf, _ = get_data(filtered=True, normalize=x_data_scalers)\n",
    "_, y_test, yhatf = get_model_predictions(model,x_dataf,y_dataf, x_data_scalers)\n",
    "resultsDict['LSTM-F-SL (Filtered)'] = evaluate(y_test, yhatf)\n",
    "predictionsDict['LSTM-F-SL (Filtered)'] = yhatf\n",
    "gtDict['LSTM-F-SL (Filtered)'] = y_test\n",
    "\n",
    "#########################################################################\n",
    "\n",
    "model =  tf.keras.models.load_model(\"models/Sri Lanka_LSTM_Unfiltered.h5\")\n",
    "\n",
    "x_data, y_data, _ = get_data(filtered=False, normalize=x_data_scalers)\n",
    "_, y_test, y_pred_raw = get_model_predictions(model,x_data,y_data,x_data_scalers)\n",
    "resultsDict['LSTM-R-SL (Raw)'] = evaluate(y_test, y_pred_raw)\n",
    "predictionsDict['LSTM-R-SL (Raw)'] = y_pred_raw\n",
    "gtDict['LSTM-R-SL (Raw)'] = y_test\n",
    "\n",
    "#########################################################################\n",
    "\n",
    "# model =  tf.keras.models.load_model(\"models/Texas_LSTM_Filtered.h5\")\n",
    "\n",
    "# x_dataf, y_dataf, _ = get_data(filtered=True, normalize=x_data_scalers)\n",
    "# _, y_test, y_pred_rawtex = get_model_predictions(model,x_dataf,y_dataf,x_data_scalers)\n",
    "# resultsDict['LSTM-F-Texas (Filtered)'] = evaluate(y_test, y_pred_rawtex)\n",
    "# predictionsDict['LSTM-F-Texas (Filtered)'] = y_pred_rawtex\n",
    "# gtDict['LSTM-F-Texas (Filtered)'] = y_test\n",
    "\n",
    "#########################################################################\n",
    "\n",
    "x_data, y_data = get_data(filtered=False, normalize=False)\n",
    "x_dataf, y_dataf = get_data(filtered=True, normalize=False)\n",
    "X = np.expand_dims(x_data[split_days-14:split_days,:],0)\n",
    "Xf = np.expand_dims(x_dataf[split_days-14:split_days,:],0)\n",
    "# X = np.expand_dims(x_data[:split_days,:],0)\n",
    "# Xf = np.expand_dims(x_dataf[:split_days,:],0)\n",
    "Y = y_data[split_days-1:,:]\n",
    "Yf = y_dataf[split_days-1:,:]\n",
    "\n",
    "Ys = [Y,\n",
    "      y_pred_raw,\n",
    "      yhatf,\n",
    "      yhat,\n",
    "     ]\n",
    "method_list = ['Observations Raw',\n",
    "               'Method A',\n",
    "               'Method B', \n",
    "               'Method C',\n",
    "               ]\n",
    "styles = {\n",
    "    'X':{'Preprocessing':'Raw','Data':'Training', 'Size':2},\n",
    "    'Xf':{'Preprocessing':'Filtered','Data':'Training', 'Size':2},\n",
    "    'Observations Raw':{'Preprocessing':'Raw','Data':'Training', 'Size':2},\n",
    "    'Method A':{'Preprocessing':'Raw','Data':'Method A', 'Size':4},\n",
    "    'Method B':{'Preprocessing':'Filtered','Data':'Method B', 'Size':3},\n",
    "    'Method C':{'Preprocessing':'Raw','Data':'Method C', 'Size':4},\n",
    "}\n",
    "          \n",
    "\n",
    "for i in range(len(Ys)):\n",
    "    print(method_list[i],Ys[i].shape)\n",
    "    Ys[i] = np.expand_dims(Ys[i],0)\n",
    "Ys = np.stack(Ys, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region_mask = ((200 > np.mean(x_data,0)) * (np.mean(x_data,0) > 140)).astype('int32')\n",
    "region_mask = (np.arange(n_regions) == 4).astype('int32')\n",
    "\n",
    "plt.figure(figsize=(18,9))\n",
    "\n",
    "plot_prediction(X,Xf, Ys, method_list, styles, region_names, region_mask)\n",
    "\n",
    "plt.savefig(f\"images/{DATASET}_DayByDay.eps\")\n",
    "plt.savefig(f\"images/{DATASET}_DayByDay.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model prediction evolution from given only last 14 days of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_model_predictions(model,x_data,y_data, scalers):\n",
    "    WINDOW_LENGTH = model.input.shape[1] \n",
    "    PREDICT_STEPS = model.output.shape[1]\n",
    "    print(f\"Predicting from model. X={x_data.shape} Y={y_data.shape}\")\n",
    "    X_test_w = x_data[split_days-WINDOW_LENGTH-1:split_days-1, :]\n",
    "    y_test_w = y_data[split_days-1:, :]\n",
    "    X_test_w = np.expand_dims(X_test_w.T,-1) #  shape = regions (samples), window size, 1\n",
    "    print(X_test_w.shape, y_test_w.shape)\n",
    "\n",
    "    yhat = []\n",
    "    for day in range(split_days-1, x_data.shape[0]):\n",
    "        y_pred = model.predict(X_test_w)\n",
    "\n",
    "        X_test_w[:,:-1,:] = X_test_w[:,1:,:] \n",
    "        X_test_w[:,-1,:] = y_pred[:, 0:1]\n",
    "\n",
    "        yhat.append(y_pred[:,0])\n",
    "\n",
    "\n",
    "    yhat = np.squeeze(np.array(yhat))\n",
    "    print(yhat.shape,y_test_w.shape)\n",
    "\n",
    "    yhat = undo_normalization(yhat, scalers)[0]\n",
    "    y_test_w = undo_normalization(y_test_w, scalers)[0]\n",
    "\n",
    "    return X_test_w, y_test_w, yhat\n",
    "\n",
    "#########################################################################\n",
    "\n",
    "model =  tf.keras.models.load_model(\"models/Sri Lanka_LSTM_Filtered.h5\")\n",
    "\n",
    "x_data, y_data, _ = get_data(filtered=False, normalize=x_data_scalers)\n",
    "_, y_test, yhat = get_model_predictions(model,x_data,y_data, x_data_scalers)\n",
    "resultsDict['ELSTM-F-SL (Raw)'] = evaluate(y_test, yhat)\n",
    "predictionsDict['ELSTM-F-SL (Raw)'] = yhat\n",
    "\n",
    "x_dataf, y_dataf, _ = get_data(filtered=True, normalize=x_data_scalers)\n",
    "_, y_test, yhatf = get_model_predictions(model,x_dataf,y_dataf, x_data_scalersf)\n",
    "resultsDict['ELSTM-F (Filtered)'] = evaluate(y_test, yhatf)\n",
    "predictionsDict['ELSTM-F-SL (Filtered)'] = yhatf\n",
    "\n",
    "#########################################################################\n",
    "\n",
    "model =  tf.keras.models.load_model(\"models/Sri Lanka_LSTM_Unfiltered.h5\")\n",
    "\n",
    "x_data, y_data, _ = get_data(filtered=False, normalize=x_data_scalers)\n",
    "_, y_test, y_pred_raw = get_model_predictions(model,x_data,y_data,x_data_scalers)\n",
    "resultsDict['ELSTM-R-SL (Raw)'] = evaluate(y_test, y_pred_raw)\n",
    "predictionsDict['ELSTM-R-SL (Raw)'] = y_pred_raw\n",
    "\n",
    "#########################################################################\n",
    "\n",
    "# model =  tf.keras.models.load_model(\"models/Texas_LSTM_Filtered.h5\")\n",
    "\n",
    "# x_data, y_data, _ = get_data(filtered=False, normalize=x_data_scalers)\n",
    "# _, y_test, y_pred_rawtex = get_model_predictions(model,x_data,y_data,x_data_scalers)\n",
    "# resultsDict['ELSTM-F-T (Raw)'] = evaluate(y_test, y_pred_rawtex)\n",
    "# predictionsDict['ELSTM-F-T (Raw)'] = y_pred_rawtex\n",
    "\n",
    "#########################################################################\n",
    "\n",
    "x_data, y_data = get_data(filtered=False, normalize=False)\n",
    "x_dataf, y_dataf = get_data(filtered=True, normalize=False)\n",
    "X = np.expand_dims(x_data[split_days-14:split_days,:],0)\n",
    "Xf = np.expand_dims(x_dataf[split_days-14:split_days,:],0)\n",
    "# X = np.expand_dims(x_data[:split_days,:],0)\n",
    "# Xf = np.expand_dims(x_dataf[:split_days,:],0)\n",
    "Y = y_data[split_days-1:,:]\n",
    "Yf = y_dataf[split_days-1:,:]\n",
    "\n",
    "Ys = [Y,\n",
    "      y_pred_raw,\n",
    "      yhatf,\n",
    "      yhat,\n",
    "     ]\n",
    "method_list = ['Observations Raw',\n",
    "               'Method A',\n",
    "               'Method B', \n",
    "               'Method C',\n",
    "               ]\n",
    "\n",
    "styles = {\n",
    "    'X':{'Preprocessing':'Raw','Data':'Training', 'Size':2},\n",
    "    'Xf':{'Preprocessing':'Filtered','Data':'Training', 'Size':2},\n",
    "    'Observations Raw':{'Preprocessing':'Raw','Data':'Training', 'Size':2},\n",
    "    'Method A':{'Preprocessing':'Raw','Data':'Method A', 'Size':4},\n",
    "    'Method B':{'Preprocessing':'Filtered','Data':'Method B', 'Size':3},\n",
    "    'Method C':{'Preprocessing':'Raw','Data':'Method C', 'Size':4},\n",
    "}\n",
    "\n",
    "for i in range(len(Ys)):\n",
    "    print(method_list[i],Ys[i].shape)\n",
    "    Ys[i] = np.expand_dims(Ys[i],0)\n",
    "Ys = np.stack(Ys, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region_mask = (np.mean(x_data,0) > 140).astype('int32')\n",
    "region_mask = (np.arange(n_regions) == 4).astype('int32')\n",
    "\n",
    "plt.figure(figsize=(18,9))\n",
    "plot_prediction(X,Xf, Ys, method_list, styles, region_names, region_mask)\n",
    "\n",
    "plt.savefig(f\"images/{DATASET}_Evolution.eps\")\n",
    "plt.savefig(f\"images/{DATASET}_Evolution.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 14\n",
    "start_seqs = [np.random.random((1,x,1))*0,\n",
    "              np.ones((1,x,1))*0,\n",
    "              np.ones((1,x,1))*0.5,\n",
    "              np.ones((1,x,1))*1,\n",
    "              np.arange(x).reshape((1,x,1))/30,\n",
    "              np.sin(np.arange(x)/x*np.pi/2).reshape((1,x,1))\n",
    "]\n",
    "model =  tf.keras.models.load_model(\"models/Sri Lanka_LSTM_Filtered.h5\")\n",
    "predictions = []\n",
    "for start_seq in start_seqs:\n",
    "    input_seq = np.copy(start_seq)\n",
    "    predict_seq = [start_seq[0,:,0]]\n",
    "    for _ in range(50):\n",
    "        output = model(input_seq, training=False)\n",
    "        predict_seq.append(output[0])\n",
    "        input_seq = input_seq[:,output.shape[1]:,:]\n",
    "        input_seq = np.append(input_seq, output).reshape((1,-1,1))\n",
    "    predictions.append(np.concatenate(predict_seq))\n",
    "plt.plot(np.array(predictions).T)\n",
    "plt.show()\n",
    "\n",
    "model =  tf.keras.models.load_model(\"models/Sri Lanka_LSTM_Unfiltered.h5\")\n",
    "predictions = []\n",
    "for start_seq in start_seqs:\n",
    "    input_seq = np.copy(start_seq)\n",
    "    predict_seq = [start_seq[0,:,0]]\n",
    "    for _ in range(50):\n",
    "        output = model(input_seq, training=False)\n",
    "        predict_seq.append(output[0])\n",
    "        input_seq = input_seq[:,output.shape[1]:,:]\n",
    "        input_seq = np.append(input_seq, output).reshape((1,-1,1))\n",
    "    predictions.append(np.concatenate(predict_seq))\n",
    "plt.plot(np.array(predictions).T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8wmAcNHSMkM"
   },
   "source": [
    "#### DeepAR - Pandas version mismatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qdqBrYtSMkN"
   },
   "source": [
    "[DeepAR](https://arxiv.org/pdf/1704.04110.pdf) is a deep learning architecture released by amazon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RDFhdm1aSMkN"
   },
   "outputs": [],
   "source": [
    "# features = ['COL']\n",
    "# # X_train = df_training.values\n",
    "\n",
    "# # scaler = StandardScaler() \n",
    "# # scaler.fit(X_train) #No cheating, never scale on the training+test!\n",
    "# # df_training[features] = scaler.transform(df_training[features])  \n",
    "# # df_test[features] = scaler.transform(df_test[features])\n",
    "\n",
    "\n",
    "# training_data = ListDataset(\n",
    "#     [{\"start\": df_training.index[0], \"target\": df_training.COL,\n",
    "#       'feat_dynamic_real': [df_training[feature] for feature in features]\n",
    "#       }],\n",
    "#     freq=\"d\"\n",
    "# )\n",
    "# test_data = ListDataset(\n",
    "#     [{\"start\": df_test.index[0], \"target\": df_test.COL,\n",
    "#       'feat_dynamic_real': [df_test[feature] for feature in features]\n",
    "#       }],\n",
    "#     freq=\"d\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GsqC4VHkSMkN"
   },
   "outputs": [],
   "source": [
    "# estimator = DeepAREstimator(freq=\"d\",\n",
    "#                             prediction_length=1\n",
    "#                             , context_length=30,\n",
    "#                             trainer=Trainer(epochs=5))\n",
    "\n",
    "# predictor = estimator.train(training_data=training_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# forecast_it, ts_it = make_evaluation_predictions(test_data, predictor=predictor, num_samples=len(df_test))\n",
    "\n",
    "# forecasts = list(forecast_it)\n",
    "# tss = list(ts_it)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yY45Bz8uSMkN"
   },
   "outputs": [],
   "source": [
    "# yhat = forecasts[0].samples.reshape(1,-1)[0]\n",
    "# resultsDict['DeepAR'] = evaluate(y_test,yhat)\n",
    "# predictionsDict['DeepAR'] = yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pw5xgetaSMkN"
   },
   "outputs": [],
   "source": [
    "metric = 'mae'\n",
    "\n",
    "arr = []\n",
    "for method in resultsDict.keys():\n",
    "    arr.append([])\n",
    "    for dist in resultsDict[method].keys():\n",
    "        arr[-1].append(resultsDict[method][dist][metric])\n",
    "\n",
    "arr = np.array(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4H1O70XnSMkN",
    "outputId": "6d81f0e0-3605-4e5d-9728-dacba49357a4"
   },
   "outputs": [],
   "source": [
    "plt.imshow(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jgg9Ld06SMkN",
    "outputId": "9030486a-8e38-4184-c22d-123ef85b06dc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.arange(len(arr[0]))\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "w = 1/len(arr)\n",
    "for i in range(len(arr)):\n",
    "    ax.bar(X + w*i, arr[i,:],  width =w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metric = 'mae'\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "arr = []\n",
    "methods = []\n",
    "# methods = ['Naive mean','LSTM-F-SL (R-R)','LSTM-F-SL (F-F)','LSTM-R-SL (R-R)','LSTM-F-SL (F-R)']\n",
    "methods = ['Method A','Method B','Method C']\n",
    "i=0\n",
    "for method in ['LSTM-R-SL (Raw)','LSTM-F-SL (Filtered)','LSTM-F-SL (Raw)']:\n",
    "    if method==\"Yesterdays value\" :\n",
    "        continue\n",
    "    err = predictionsDict[method] - gtDict['LSTM-F-SL (Raw)'] \n",
    "    abserr = np.abs(err)\n",
    "    sqderr = err**2\n",
    "    mape = (abserr/(gtDict['LSTM-F-SL (Raw)']+predictionsDict[method])*100)\n",
    "    \n",
    "    arr.append(abserr)\n",
    "    methods.append(method)\n",
    "    \n",
    "    n, bins, patches = plt.hist(abserr.reshape(-1), 1000, density=True, histtype='step',\n",
    "                           cumulative=True, label=methods[i])\n",
    "    i+=1\n",
    "    \n",
    "    patches[0].set_xy(patches[0].get_xy()[:-1])\n",
    "    \n",
    "    \n",
    "    print(method)\n",
    "    print(f'{np.mean(abserr):.2f} {np.mean(sqderr)**0.5:.2f} {np.mean(mape):.2f}')\n",
    "    \n",
    "# err = predictionsDict['LSTM-F-SL (Filtered)'] - gtDict['LSTM-F-SL (Raw)'] \n",
    "# abserr = np.abs(err)\n",
    "# sqderr = err**2\n",
    "# print(np.mean(abserr),np.mean(sqderr)**0.5)\n",
    "\n",
    "# arr.append(abserr)\n",
    "# methods.append('ll')\n",
    "# n, bins, patches =plt.hist(abserr.reshape(-1), 1000, density=True, histtype='step',\n",
    "#                            cumulative=True, label=methods[-1])\n",
    "# patches[0].set_xy(patches[0].get_xy()[:-1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel(\"Absolute error\")\n",
    "plt.ylabel(\"Cumulative probability density\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{0.223:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(arr).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.array(arr).reshape((-1,np.array(arr).shape[-1])).T, 1000, density=True, histtype='step',\n",
    "                           cumulative=True, label='Empirical')\n",
    "plt.legend(region_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5WKfBEYSMkO",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7A4PCPB1SMkO",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6To6oy6SMkO"
   },
   "source": [
    "We have seen models with really low amount of parameters (Auto regression models,Linear models) or with crazy ammount (Trees,Prophet). Some models are more robust to different data types/shapes and dont need any hyperparameter optimization but some other can give you poor results if the parameters are not tunned, we can tune the model parameters to better fit our dataset properties. We can do this manually with pure knowledge about the model but this becames really hard when the model contains a lot of different parameters, this is when hyperparameter optimization comes handy.\n",
    "\n",
    "Hyperparameter optimization is trying to find the best parameters in an automatic way. We present two methods that are used frequently:\n",
    "\n",
    "* **Grid search** Brute force method to try all different possible combinations of parameters. Will always find the best combination\n",
    "* **Bayesian processes** \"Brute\" force method, optimizes parameter search by using gausian processes to model each parameter distribution and don't go over all the possible values. Really nice library for python https://github.com/fmfn/BayesianOptimization, this method will not always find the best combination of parameters\n",
    "\n",
    "We provide 1 example for each method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5mEz-GdSMkO"
   },
   "source": [
    "### Grid search - SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXgOp_g5SMkO"
   },
   "source": [
    "With grid search we can use the handy sklearn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDHq0IMUSMkO"
   },
   "outputs": [],
   "source": [
    "reg = GridSearchCV(svm.SVR(kernel='rbf', gamma=0.1),\n",
    "                   param_grid={\"C\": [1e0, 1e1, 1e2, 1e3],\n",
    "                               \"gamma\": np.logspace(-2, 2, 5)})\n",
    "reg.fit(X_train, y_train)\n",
    "yhat = reg.predict(X_test)\n",
    "resultsDict['SVM RBF GRID SEARCH'] = evaluate(df_test.pollution_today, yhat)\n",
    "predictionsDict['SVM RBF GRID SEARCH'] = yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XnF54r19SMkO",
    "outputId": "0199a5b3-b3f4-40c7-f953-5452fbb82c54"
   },
   "outputs": [],
   "source": [
    "increase = 1 - (resultsDict['SVM RBF GRID SEARCH']['rmse']/resultsDict['SVM RBF']['rmse'])\n",
    "print(f\"Grid search Tunned SVM is {increase*100}% better than the SVM with default parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldfTunivSMkP"
   },
   "source": [
    "### Bayesian processes - Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBfaJrueSMkP"
   },
   "outputs": [],
   "source": [
    "def rms(y_actual,y_predicted):\n",
    "    return sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "\n",
    "my_scorer = make_scorer(rms, greater_is_better=False)\n",
    "pbounds = {\n",
    "    'n_estimators': (100, 10000),\n",
    "    'max_depth': (3,15),\n",
    "    'min_samples_leaf': (1,4),\n",
    "    'min_samples_split': (2,10),\n",
    "}\n",
    " \n",
    "def rf_hyper_param(n_estimators,\n",
    "                   max_depth,\n",
    "                   min_samples_leaf,\n",
    "                   min_samples_split):\n",
    " \n",
    "    max_depth = int(max_depth)\n",
    "    n_estimators = int(n_estimators)\n",
    " \n",
    "    clf = RandomForestRegressor(n_estimators=n_estimators, \n",
    "                                max_depth=int(max_depth),\n",
    "                                min_samples_leaf=int(min_samples_leaf),\n",
    "                                min_samples_split=int(min_samples_split),\n",
    "                                n_jobs=1)\n",
    "    \n",
    "    return -np.mean(cross_val_score(clf, X_train, y_train, cv=3))\n",
    " \n",
    "optimizer = BayesianOptimization(\n",
    "    f=rf_hyper_param,\n",
    "    pbounds=pbounds,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKx7t9NHSMkP",
    "outputId": "c434c0a3-78bc-480a-886d-e55271acb1e4"
   },
   "outputs": [],
   "source": [
    "optimizer.maximize(\n",
    "    init_points=3,\n",
    "    n_iter=20,\n",
    "    acq='ei'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s39DQ_m3SMkP"
   },
   "outputs": [],
   "source": [
    "params = optimizer.max['params']\n",
    "\n",
    "#Converting the max_depth and n_estimator values from float to int\n",
    "params['max_depth']= int(params['max_depth'])\n",
    "params['n_estimators']= int(params['n_estimators'])\n",
    "params['min_samples_leaf']= int(params['min_samples_leaf'])\n",
    "params['min_samples_split']= int(params['min_samples_split'])\n",
    "\n",
    "#Initialize an XGBRegressor with the tuned parameters and fit the training data\n",
    "tunned_rf = RandomForestRegressor(**params)\n",
    "tunned_rf.fit(X_train, y_train) # Change verbose to True if you want to see it train\n",
    "\n",
    "yhat = tunned_rf.predict(X_test)\n",
    "resultsDict['Randomforest tunned'] = evaluate(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDwOY8-QSMkP",
    "outputId": "391b5b20-5a42-4a37-92f0-fdac53f7eca8"
   },
   "outputs": [],
   "source": [
    "increase = 1 - (resultsDict['Randomforest tunned']['rmse']/resultsDict['Randomforest']['rmse'])\n",
    "print(f\"Bayesian optimized Randomforest is {increase*100}% better than the Randomforest with default parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGW8Jvs_SMkP"
   },
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLH6TdzRSMkQ"
   },
   "source": [
    "Ensembling refers to combine multiple models to achieve a better performance, most of the time this only makes sense when models have similar performance but predict values differently so we try to get the best of each model.\n",
    "\n",
    "We will pick our 3 top performing models and look at the correlation of their residuals, the less correlated the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmsHiBzUSMkQ",
    "outputId": "f8082cbd-2ccc-4e2f-d0dc-01ac58def3fb"
   },
   "outputs": [],
   "source": [
    "models = ['Tensorflow simple LSTM',\n",
    " 'Lightgbm',\n",
    " 'XGBoost']\n",
    "resis = pd.DataFrame(data={k: df_test.pollution_today.values - v for k, v in predictionsDict.items()})[models]\n",
    "corr = resis.corr()\n",
    "print(\"Residuals correlation\")\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2zDORsASMkQ"
   },
   "source": [
    "We can see how both tree models are a bit similar ~0.87 but quite different from the Deep Learning model with corr ~0.7. In this case it would really make sense to ensemble the methods and see how they behave. The most reasonable combinations to try would be\n",
    "\n",
    "* XGboost + Tensorflow\n",
    "* XGBoost + Lightgbm\n",
    "* Lightgbm + Tensorflow\n",
    "* XGBoost + Lightgbm + Tensorflow\n",
    "\n",
    "We will just sum the predictions of each model with similar weights (0.5 if two models, 0.333 if three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ynHoyC4JSMkQ"
   },
   "outputs": [],
   "source": [
    "predictionsDict['EnsembleXG+LIGHT'] = (predictionsDict['XGBoost'] + predictionsDict['Lightgbm'])/2\n",
    "resultsDict['EnsembleXG+LIGHT'] = evaluate(df_test.pollution_today.values,predictionsDict['EnsembleXG+LIGHT'])\n",
    "\n",
    "predictionsDict['EnsembleXG+LIGHT+TF'] = (predictionsDict['XGBoost'] + predictionsDict['Lightgbm']+ predictionsDict['Tensorflow simple LSTM'])/3\n",
    "resultsDict['EnsembleXG+LIGHT+TF'] = evaluate(df_test.pollution_today.values,predictionsDict['EnsembleXG+LIGHT+TF'])\n",
    "\n",
    "predictionsDict['EnsembleLIGHT+TF'] = (predictionsDict['Lightgbm']+ predictionsDict['Tensorflow simple LSTM'])/2\n",
    "resultsDict['EnsembleLIGHT+TF'] = evaluate(df_test.pollution_today.values,predictionsDict['EnsembleLIGHT+TF'])\n",
    "\n",
    "predictionsDict['EnsembleXG+TF'] = (predictionsDict['XGBoost']+ predictionsDict['Tensorflow simple LSTM'])/2\n",
    "resultsDict['EnsembleXG+TF'] = evaluate(df_test.pollution_today.values,predictionsDict['EnsembleXG+TF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PP4BEk0GSMkQ"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('results/scores.pickle', 'wb') as handle:\n",
    "    pickle.dump(resultsDict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('results/predictions.pickle', 'wb') as handle:\n",
    "    pickle.dump(predictionsDict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "j8EQyYY9SMj-",
    "mUDzGC1vSMj_",
    "PgTcxmY1SMj_",
    "93XlgV2iSMkA",
    "20lsAGAASMkB",
    "dsCiivzZSMkC",
    "jlgpwL74SMkC",
    "CR21Ilh6SMkD",
    "ipoOGE5gSMkE",
    "FJ3mtfZuSMkF",
    "9GwAWbuiSMkF",
    "0OQ5QEzTSMkH",
    "sWifPcYLSMkH",
    "cbSEXc4VSMkH",
    "xJbwJvwNSMkH",
    "XLj27kBBSMkI",
    "Pg6q5UwKSMkI",
    "NiU3GWOQSMkI",
    "zEHrkHKoSMkJ",
    "3KKwh6cjSMkJ",
    "EBaC48IKSMkJ",
    "I8wmAcNHSMkM",
    "s5mEz-GdSMkO",
    "ldfTunivSMkP"
   ],
   "name": "forecasting.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
