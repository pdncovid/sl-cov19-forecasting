{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# import sys\n",
    "# import os\n",
    "# path = \"/content/drive/Shareddrives/covid.eng.pdn.ac.lk drive/COVID-AI (PG)/spatio_temporal/Covid19_DL_Forecasting_Codes\"\n",
    "# os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "sys.path.insert(0, os.path.join(sys.path[0], '..'))\n",
    "import pandas as pd  # Basic library for all of our dataset operations\n",
    "import numpy as np\n",
    "import requests\n",
    "import io\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels as sm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import pmdarima as pm\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "\n",
    "import lightgbm as lgb\n",
    "# import gluonts\n",
    "from math import sqrt\n",
    "\n",
    "# data manipulation and signal processing\n",
    "import math\n",
    "import scipy\n",
    "from scipy import signal\n",
    "import scipy.stats as ss\n",
    "\n",
    "# plots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import folium\n",
    "\n",
    "# path = \"/con\n",
    "\n",
    "import shap\n",
    "warnings.filterwarnings(\"ignore\") #We will use deprecated models of statmodels which throw a lot of warnings to use more modern ones\n",
    "\n",
    "from utils.metrics import evaluate\n",
    "from utils.plots import bar_metrics, plot_prediction\n",
    "from utils.functions import distance, normalize_for_nn, undo_normalization\n",
    "from utils.data_loader import load_data, per_million, get_daily\n",
    "from utils.smoothing_functions import O_LPF,NO_LPF,O_NDA,NO_NDA\n",
    "from utils.data_splitter import split_on_region_dimension, split_on_time_dimension,split_into_pieces_inorder\n",
    "\n",
    "from random import random\n",
    "from datetime import datetime\n",
    "from sklearn import linear_model, svm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from math import sqrt\n",
    "from itertools import islice\n",
    "from pylab import rcParams\n",
    "# progress bar\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "\n",
    "#Extra settings\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "plt.style.use('bmh')\n",
    "mpl.rcParams['axes.labelsize'] = 14\n",
    "mpl.rcParams['xtick.labelsize'] = 12\n",
    "mpl.rcParams['ytick.labelsize'] = 12\n",
    "mpl.rcParams['text.color'] = 'k'\n",
    "mpl.rcParams['figure.figsize'] = 18, 8\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required variables:\n",
    "\n",
    "*   **region_names** - Names of the unique regions.\n",
    "*   **confirmed_cases** - 2D array. Each row should corresponds to values in 'region_names'. Each column represents a day. Columns should be in ascending order. (Starting day -> Present)\n",
    "*   **daily_cases** - confirmed_cases.diff()\n",
    "*   **population** - Population in 'region'\n",
    "*   **features** - Features of the regions. Each column is a certain feature.\n",
    "*   **START_DATE** - Starting date of the data DD/MM/YYYY\n",
    "*   **n_regions** Number of regions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_data = True\n",
    "DATASET = \"Sri Lanka\" #\"Sri Lanka\" \"Texas\" \"USA\" \"Global\"\n",
    "split_date = '2021-02-01'\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 16\n",
    "BUFFER_SIZE = 100\n",
    "WINDOW_LENGTH = 14\n",
    "PREDICT_STEPS = 7\n",
    "lr = 0.002\n",
    "TRAINING_DATA_TYPE = \"Filtered\" #\"Filtered\"\n",
    "\n",
    "\n",
    "d = load_data(DATASET,path=\"../Datasets\")\n",
    "region_names=d[\"region_names\"] \n",
    "confirmed_cases=d[\"confirmed_cases\"] \n",
    "daily_cases=d[\"daily_cases\"] \n",
    "features=d[\"features\"] \n",
    "START_DATE=d[\"START_DATE\"] \n",
    "n_regions=d[\"n_regions\"] \n",
    "daily_cases[daily_cases<0] = 0\n",
    "population = features[\"Population\"]\n",
    "for i in range(len(population)):\n",
    "    print(\"{:.2f}%\".format(confirmed_cases[i,:].max()/population[i]*100), region_names[i])\n",
    "\n",
    "days = confirmed_cases.shape[1]\n",
    "n_features = features.shape[1]\n",
    "\n",
    "print(f\"Total population {population.sum()/1e6:.2f}M, regions:{n_regions}, days:{days}\")\n",
    "\n",
    "daily_filtered = O_LPF(daily_cases, datatype='daily', order=3, R_weight=1.0, EIG_weight=1, corr = True, region_names=region_names)\n",
    "\n",
    "daily_per_mio_capita = per_million(daily_cases,population)\n",
    "daily_per_mio_capita_filtered = per_million(daily_filtered,population)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(daily_cases.T,columns = features.index)\n",
    "df.index = pd.to_datetime(pd.to_datetime(START_DATE).value + df.index*24*3600*1000000000)\n",
    "\n",
    "\n",
    "df_training = df.loc[df.index <= split_date]\n",
    "df_test = df.loc[df.index > split_date]\n",
    "print(f\"{len(df_training)} days of training data \\n {len(df_test)} days of testing data \")\n",
    "\n",
    "df_training.to_csv('../Datasets/training.csv')\n",
    "df_test.to_csv('../Datasets/test.csv')\n",
    "\n",
    "split_days = (pd.to_datetime(split_date)-pd.to_datetime(START_DATE)).days\n",
    "\n",
    "df_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Dense Models***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_regions2batch = True\n",
    "MODEL_TYPE = \"DENSE\"\n",
    "def get_model_ANN(seq_size, predict_steps, n_features=n_features, n_regions=n_regions):\n",
    "    \n",
    "    inp_seq = tf.keras.layers.Input(seq_size, name=\"input_sequence\")\n",
    "    inp_fea = tf.keras.layers.Input(n_features, name=\"input_features\")\n",
    "    \n",
    "    x = inp_seq\n",
    "    xf = inp_fea\n",
    "    n = n_features\n",
    "    while (n>0):\n",
    "        xf = tf.keras.layers.Dense(n,activation='relu')(xf) \n",
    "        n = n//2\n",
    "    \n",
    "    x = tf.keras.layers.Dense(10, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(predict_steps,activation='relu')(x)\n",
    "    \n",
    "    if n_features > 0:\n",
    "        x = x*xf\n",
    "    model = tf.keras.models.Model([inp_seq,inp_fea], x)\n",
    "    return model\n",
    "\n",
    "model = get_model_ANN(WINDOW_LENGTH, PREDICT_STEPS)\n",
    "model.summary()\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, rankdir='LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_regions2batch = False\n",
    "MODEL_TYPE = \"DENSE\"\n",
    "def get_model_ANN(input_seq_size, output_seq_size, n_features=n_features, n_regions=n_regions):\n",
    "    \n",
    "    inp_seq = tf.keras.layers.Input((input_seq_size,n_regions), name=\"input_sequence\")\n",
    "    inp_fea = tf.keras.layers.Input((n_features,n_regions), name=\"input_features\")\n",
    "    \n",
    "    x = tf.keras.layers.Reshape((input_seq_size*n_regions,))(inp_seq)\n",
    "    x = tf.keras.layers.Dense(output_seq_size*n_regions, activation='sigmoid')(x)\n",
    "    x = tf.keras.layers.Reshape((output_seq_size,n_regions))(x)\n",
    "\n",
    "    model = tf.keras.models.Model([inp_seq, inp_fea], x)\n",
    "    return model\n",
    "model = get_model_ANN(WINDOW_LENGTH, PREDICT_STEPS, n_regions=n_regions)\n",
    "model.summary()\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***LSTM MODEL***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_regions2batch = True\n",
    "MODEL_TYPE = \"LSTM\"\n",
    "dropout=0.5\n",
    "def get_model_LSTM(input_seq_size, output_seq_size, n_features=features.shape[1], n_regions=n_regions):\n",
    "    \n",
    "    inp_seq = tf.keras.layers.Input((input_seq_size,1), name=\"input_seq\")\n",
    "#     inp_fea = tf.keras.layers.Input(n_features, name=\"input_fea\")\n",
    "    \n",
    "    x = inp_seq\n",
    "#     x = tf.keras.layers.LSTM(32, activation='relu', return_sequences=True)(x)\n",
    "    x = tf.keras.layers.LSTM(output_seq_size)(x)\n",
    "    x = tf.keras.layers.Activation('sigmoid')(x)\n",
    "    model = tf.keras.models.Model(inp_seq, x)\n",
    "    \n",
    "    \n",
    "#     print(\"Input shape\", X_train.shape[-2:])\n",
    "#     model = tf.keras.models.Sequential([\n",
    "#         tf.keras.layers.LSTM(128, input_shape=X_train.shape[-2:],dropout=dropout),\n",
    "#         tf.keras.layers.Dense(128),\n",
    "#         tf.keras.layers.Dense(output_seq_size),\n",
    "#         tf.keras.layers.Activation('sigmoid')\n",
    "#     ])\n",
    "\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model_LSTM(WINDOW_LENGTH, PREDICT_STEPS)\n",
    "\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, rankdir='LR')\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "# logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") #Support for tensorboard tracking!\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)**LSTM MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_regions2batch = False\n",
    "MODEL_TYPE = \"LSTM\"\n",
    "def get_model_LSTM(input_seq_size, output_seq_size, n_regions):\n",
    "    \n",
    "    inp_seq = tf.keras.layers.Input((input_seq_size,n_regions), name=\"input_seq\")\n",
    "    x = tf.keras.layers.LSTM(output_seq_size*n_regions, activation='sigmoid')(inp_seq)\n",
    "    x = tf.keras.layers.Reshape((output_seq_size, n_regions))(x)\n",
    "    \n",
    "#     x = tf.keras.layers.Activation('sigmoid')(x)\n",
    "    model = tf.keras.models.Model(inp_seq, x)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_LSTM = get_model_LSTM(WINDOW_LENGTH, PREDICT_STEPS, n_regions)\n",
    "model_LSTM.summary()\n",
    "tf.keras.utils.plot_model(model_LSTM, show_shapes=True, rankdir='LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_regions2batch = False\n",
    "MODEL_TYPE = \"LSTM\"\n",
    "def get_model_LSTM(input_seq_size, output_seq_size, n_regions):\n",
    "    \n",
    "    inp_seq = tf.keras.layers.Input((input_seq_size,n_regions), name=\"input_seq\")\n",
    "    \n",
    "    lstm_input = inp_seq\n",
    "    for i in range(output_seq_size):\n",
    "        xx = tf.keras.layers.LSTM(n_regions, activation='relu')(lstm_input)\n",
    "        out = xx if i==0 else tf.keras.layers.concatenate([out,xx])\n",
    "       \n",
    "        xx = tf.reshape(xx,(-1,1,n_regions))\n",
    "        \n",
    "        lstm_input = tf.keras.layers.concatenate([lstm_input[:,1:,:], xx], axis=1)\n",
    "    \n",
    "    out = tf.reshape(out,(-1,output_seq_size,n_regions))\n",
    "#     out = tf.keras.layers.Activation('sigmoid')(out)\n",
    "    model = tf.keras.models.Model(inp_seq, out)\n",
    "    return model\n",
    "\n",
    "model = get_model_LSTM(WINDOW_LENGTH, PREDICT_STEPS, n_regions)\n",
    "model.summary()\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, rankdir='TB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filtered, normalize=False):\n",
    "    if filtered == False:\n",
    "        x,y= np.copy(daily_cases), np.copy(daily_cases)\n",
    "    else:\n",
    "        x,y= np.copy(daily_filtered), np.copy(daily_filtered)\n",
    "    \n",
    "    \n",
    "    x = per_million(x, population)\n",
    "    y = per_million(y, population)\n",
    "    if normalize:\n",
    "        \n",
    "        x, xs=normalize_for_nn(x, None if type(normalize)==bool else normalize)\n",
    "        y, xs=normalize_for_nn(y, xs)\n",
    "        return x.T,y.T,xs\n",
    "    else:\n",
    "        return x.T,y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x_data, y_data, x_data_scalers = get_data(False,normalize=True)\n",
    "x_dataf, y_dataf, x_data_scalersf = get_data(True, normalize=True)\n",
    "\n",
    "x_data, y_data, _ = get_data(TRAINING_DATA_TYPE==\"Filtered\",normalize=x_data_scalers)\n",
    "\n",
    "features = np.zeros((n_regions,0))\n",
    "\n",
    "\n",
    "tmp = split_on_time_dimension(x_data.T, y_data.T, features, WINDOW_LENGTH, PREDICT_STEPS, \n",
    "                              k_fold=3, test_fold=2, reduce_last_dim=reduce_regions2batch,\n",
    "                              only_train_test=True, debug=True)\n",
    "X_train, X_train_feat, Y_train, X_val, X_val_feat, Y_val, X_test, X_test_feat, Y_test = tmp\n",
    "\n",
    "if len(X_train.shape) == 2:\n",
    "    X_train, X_train_feat, Y_train = np.expand_dims(X_train,-1),np.expand_dims(X_train_feat,-1),np.expand_dims(Y_train,-1)\n",
    "    X_val, X_val_feat, Y_val = np.expand_dims(X_val,-1),np.expand_dims(X_val_feat,-1),np.expand_dims(Y_val,-1)\n",
    "    X_test, X_test_feat, Y_test = np.expand_dims(X_test,-1),np.expand_dims(X_test_feat,-1),np.expand_dims(Y_test,-1)\n",
    "\n",
    "print(\"Train\", X_train.shape, Y_train.shape, X_train_feat.shape)\n",
    "print(\"Val\", X_val.shape, Y_val.shape, X_val_feat.shape)\n",
    "print(\"Test\", X_test.shape, Y_test.shape,X_test_feat.shape)\n",
    "\n",
    "X_train = np.concatenate([X_train, X_test], 0)\n",
    "X_train_feat = np.concatenate([X_train_feat, X_test_feat], 0)\n",
    "Y_train = np.concatenate([Y_train, Y_test], 0)\n",
    "\n",
    "print(\"Train\", X_train.shape, Y_train.shape, X_train_feat.shape)\n",
    "print(\"Val\", X_val.shape, Y_val.shape, X_val_feat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def get_count(segments, data):\n",
    "    bounds = []\n",
    "    count = []\n",
    "    idx = []\n",
    "    for i in range(segments):\n",
    "        data = (data - np.amin(data))\n",
    "        bounds.append(np.round((i+1)*np.amax(data)/segments,3))\n",
    "        if i==0:\n",
    "            ineq = data <= bounds[i]\n",
    "        elif i==(segments-1):\n",
    "            ineq = data > bounds[i-1]\n",
    "        else:\n",
    "            ineq = (data > bounds[i-1])*(data <= bounds[i])\n",
    "        count.append(np.sum(ineq))\n",
    "        idx.append(np.reshape(np.array(np.where(ineq)),[-1,]))\n",
    "    count = np.array(count).astype(int)\n",
    "    bounds = np.array(bounds).astype(np.float64)\n",
    "    return count, bounds, idx\n",
    "\n",
    "dataset = df_training.values.T\n",
    "dataset_norm = np.zeros_like(dataset)\n",
    "for i in range(daily_cases.shape[0]):\n",
    "    dataset_norm[i,:] = dataset[i,:]/np.amax(dataset[i,:])\n",
    "\n",
    "alldata_train = dataset_norm\n",
    "\n",
    "samples_all = np.zeros([alldata_train.shape[0], alldata_train.shape[1]-WINDOW_LENGTH-PREDICT_STEPS, WINDOW_LENGTH])\n",
    "samples_mean = np.zeros([alldata_train.shape[0], alldata_train.shape[1]-WINDOW_LENGTH-PREDICT_STEPS])\n",
    "\n",
    "# evaluating optimal number of segments for each district\n",
    "segment_array = [2,3,4,5,6,7,8,9,10]\n",
    "segment_dist = []\n",
    "plt.figure(figsize=(5*6,5*4))\n",
    "for i in range(samples_all.shape[0]):\n",
    "    for k in range(samples_all.shape[1]):\n",
    "        samples_all[i,k,:] = alldata_train[i,k:k+WINDOW_LENGTH]\n",
    "        samples_mean[i,k] = np.mean(samples_all[i,k,:])\n",
    "    all_counts = []\n",
    "    count_score = []\n",
    "    # evaluating the count score for each district\n",
    "    for n in range(len(segment_array)):    \n",
    "        segments = segment_array[n]\n",
    "        [count, bounds, idx] = get_count(segments, samples_mean[i,:])              \n",
    "        all_counts.append(np.amin(count)*len(count))\n",
    "        count_score.append((all_counts[n]**1)*(n+1))\n",
    "    \n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.plot(segment_array,all_counts/np.amax(all_counts),linewidth=2)\n",
    "    plt.plot(segment_array,count_score/np.amax(count_score),linewidth=2)\n",
    "    plt.legend(['normalised total counts','segment score'])\n",
    "    plt.title('dist: '+region_names[i]+'  segments: '+str(segment_array[np.argmax(count_score)])+'  samples: '+str(all_counts[np.argmax(count_score)]))\n",
    "    segment_dist.append(segment_array[np.argmax(count_score)]) \n",
    "segment_dist = np.array(segment_dist).astype(int)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('segments per district= ', segment_dist)\n",
    "\n",
    "idx_rand_all = []\n",
    "for i in range(samples_all.shape[0]):\n",
    "    data = samples_mean[i,:]\n",
    "    segments = segment_dist[i]\n",
    "    [count_dist, bounds_dist, idx_dist] = get_count(segments, data)\n",
    "    n_per_seg = np.amin(count_dist)\n",
    "    data_new = []\n",
    "    idx_rand = np.zeros([segments,n_per_seg])\n",
    "    for k in range(segments):\n",
    "        idx_temp = list(idx_dist[k])\n",
    "        idx_rand[k,:] = random.sample(idx_temp,n_per_seg)\n",
    "    idx_rand = np.reshape(idx_rand, [-1,])\n",
    "    idx_rand_all.append(idx_rand)\n",
    "print(len(idx_rand_all))\n",
    "\n",
    "x_train_opt, y_train_opt = [], []\n",
    "# undersampling using optimal number of segments\n",
    "for i in range(samples_all.shape[0]):\n",
    "    data = samples_mean[i,:]\n",
    "    segments = segment_dist[i]\n",
    "    [count_dist, bounds_dist, idx_dist] = get_count(segments, data)\n",
    "    n_per_seg = np.amin(count_dist)\n",
    "    data_new = []\n",
    "    idx_rand = np.zeros([segments,n_per_seg])\n",
    "    for k in range(segments):\n",
    "        idx_temp = list(idx_dist[k])\n",
    "#         print(idx_temp)\n",
    "        idx_rand[k,:] = random.sample(idx_temp,n_per_seg)\n",
    "    idx_rand = np.reshape(idx_rand, [-1,])\n",
    "    print(region_names[i], idx_rand)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(9,3))\n",
    "    ax.plot(alldata_train[i])\n",
    "    for j,idx in enumerate(idx_rand):\n",
    "        idx=int(idx)\n",
    "        \n",
    "        rect = patches.Rectangle((idx, j/len(idx_rand)), WINDOW_LENGTH, 1/len(idx_rand), linewidth=1, edgecolor=None, facecolor=(1.,0.,0.,0.5))\n",
    "        ax.add_patch(rect)\n",
    "        rect = patches.Rectangle((idx+WINDOW_LENGTH, j/len(idx_rand)), PREDICT_STEPS, 1/len(idx_rand), linewidth=1, edgecolor=None, facecolor=(0.,1.,0.,0.5))\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        x_train_opt.append(alldata_train[i, idx:idx+WINDOW_LENGTH])\n",
    "        y_train_opt.append(alldata_train[i, idx+WINDOW_LENGTH:idx+WINDOW_LENGTH+PREDICT_STEPS])\n",
    "    plt.show()\n",
    "    \n",
    "x_train_opt = np.array(x_train_opt)\n",
    "y_train_opt = np.array(y_train_opt)\n",
    "\n",
    "x_train_opt = np.expand_dims(x_train_opt,-1)\n",
    "y_train_opt = np.expand_dims(y_train_opt,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_opt.shape,y_train_opt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if reduce_regions2batch:  # Currently optimal undersampling supports independatly on regions.\n",
    "    X_train = x_train_opt\n",
    "    Y_train = y_train_opt\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "train_data = train_data.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "test_data = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n",
    "test_data = test_data.batch(BATCH_SIZE)\n",
    "\n",
    "freq, xcheck = np.histogram(np.concatenate(X_train,-1).mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data, _ = get_data(filtered=False, normalize=True)\n",
    "plt.plot(x_data)\n",
    "plt.show()\n",
    "\n",
    "for i in range(X_train.shape[-1]):\n",
    "    plt.plot(np.concatenate([X_train[:,:,i],Y_train[:,:,i]],1).T)\n",
    "plt.axvline(X_train.shape[1],color='r',linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(X_train.reshape(-1), bins=100)\n",
    "plt.title(\"Histogram of cases\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(np.concatenate(X_train,-1).mean(0), bins=100)\n",
    "plt.title(\"Histogram of mean of training samples\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1/np.log(freq))**3*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mean Squared Error\n",
    "   \n",
    "\n",
    "# def loss_f(y_true, y_pred, x):\n",
    "#     return mse(y_true, y_pred)\n",
    "\n",
    "# Mean squared error with boosting low frequency samples\n",
    "def bs(arr, v):\n",
    "    l, r = 0, len(arr)\n",
    "    while r - l > 0:\n",
    "        i = (r + l) // 2\n",
    "        if arr[i] < v:\n",
    "            l = i+1\n",
    "        elif arr[i] > v:\n",
    "            r = i\n",
    "        else:\n",
    "            return i\n",
    "    return l\n",
    "\n",
    "def loss_f(y_true, y_pred, x):\n",
    "    region_sample_freq = np.zeros((x.shape[0], x.shape[-1]), dtype='double')\n",
    "\n",
    "    for batch in range(x.shape[0]):\n",
    "        for n in range(x.shape[-1]):\n",
    "            i = bs(xcheck, np.mean(x[batch,:,n]))-1\n",
    "            region_sample_freq[batch,n] = freq[i]\n",
    "    y_pred = tf.dtypes.cast(y_pred, tf.float64)\n",
    "    mse = tf.reduce_mean((y_true - y_pred)**2, 1)\n",
    "    \n",
    "    return tf.reduce_mean(mse*(1/np.log(region_sample_freq))**2*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmodel_name = DATASET + \"_\" + MODEL_TYPE + \"_\" + TRAINING_DATA_TYPE\n",
    "\n",
    "print(fmodel_name)\n",
    "print(\"Train\", X_train.shape, Y_train.shape, X_train_feat.shape)\n",
    "print(\"Val\", X_val.shape, Y_val.shape, X_val_feat.shape)\n",
    "print(\"Test\", X_test.shape, Y_test.shape,X_test_feat.shape)\n",
    "\n",
    "\n",
    "folder = time.strftime('%Y.%m.%d-%H.%M.%S', time.localtime())+\"_\"+fmodel_name\n",
    "os.makedirs('./logs/' + folder)\n",
    "tensorboard = TensorBoard(log_dir='./logs/' + folder, write_graph=True, histogram_freq=1, write_images=True)\n",
    "tensorboard.set_model(model)\n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr=lr)\n",
    "def eval_metric(y_true, y_pred):  \n",
    "    \n",
    "    return np.mean((np.squeeze(y_true)-np.squeeze(y_pred))**2)**0.5\n",
    "\n",
    "train_metric = []\n",
    "val_metric = []\n",
    "test_metric = []\n",
    "best_test_value = 1e10\n",
    "for epoch in range(EPOCHS):\n",
    "    losses = []\n",
    "    for x,y in train_data:\n",
    "        plt.show()\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x, training=True)\n",
    "            loss = loss_f(y, y_pred, x)\n",
    "\n",
    "        grad = tape.gradient(loss, model.trainable_variables)\n",
    "        opt.apply_gradients(zip(grad, model.trainable_variables))\n",
    "        losses.append(loss)\n",
    "\n",
    "        print(f\"\\r Epoch {epoch}: mean loss = {np.mean(losses):.5f}\", end='')\n",
    "    # add metric value of the prediction (from training data)\n",
    "    pred_train_y = model(X_train, training=False)\n",
    "    train_metric.append(eval_metric(Y_train,pred_train_y))\n",
    "    # add metric value of the prediction (from testing data)\n",
    "    pred_test_y = model(X_test, training=False)\n",
    "    test_metric.append(eval_metric(Y_test,pred_test_y))\n",
    "    \n",
    "    if test_metric[-1] < best_test_value:\n",
    "        best_test_value = test_metric[-1]\n",
    "        print(f\" Best test metric {best_test_value}. Saving model...\")\n",
    "        model.save(\"temp.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(121)\n",
    "plt.ion()\n",
    "plt.plot(train_metric, label='Train')\n",
    "plt.plot(test_metric, label='Test')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Metric\")\n",
    "plt.legend()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  tf.keras.models.load_model(\"temp.h5\")\n",
    "model.save(\"models/\"+fmodel_name+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_predictions(model,x_data,y_data, scalers):\n",
    "    print(f\"Predicting from model. X={x_data.shape} Y={y_data.shape}\")\n",
    "    # CREATING TRAIN-TEST SETS FOR CASES\n",
    "    x_test, y_test = split_into_pieces_inorder(x_data.T,y_data.T, WINDOW_LENGTH, PREDICT_STEPS, WINDOW_LENGTH+PREDICT_STEPS,\n",
    "                                               reduce_last_dim=False)\n",
    "    print(\"Input data shape\", x_test.shape)\n",
    "    if model.input.shape[-1]==1:\n",
    "        y_pred = np.zeros_like(y_test)\n",
    "        for i in range(len(region_names)):\n",
    "            y_pred[:,:,i] = model(x_test[:,:,i:i+1])\n",
    "    else:\n",
    "        y_pred = model(x_test).numpy()\n",
    "    print(\"Predicted shape\", y_pred.shape)\n",
    "    # # NOTE:\n",
    "    # # max value may change with time. then we have to retrain the model!!!!!!\n",
    "    # # we can have a predefined max value. 1 for major cities and 1 for smaller districts\n",
    "    x_test = undo_normalization(x_test, scalers)\n",
    "    y_test = undo_normalization(y_test, scalers)\n",
    "    y_pred = undo_normalization(y_pred, scalers)\n",
    "\n",
    "    return x_test, y_test, y_pred\n",
    "\n",
    "x_data, y_data, _ = get_data(filtered=False, normalize=x_data_scalers)\n",
    "x_test, y_test, y_pred = get_model_predictions(model,x_data,y_data, x_data_scalers)\n",
    "x_data, y_data, _ = get_data(filtered=True, normalize=x_data_scalers)\n",
    "x_testf, y_testf, y_predf = get_model_predictions(model,x_data,y_data, x_data_scalers)\n",
    "\n",
    "Ys = np.stack([y_test, y_testf, y_pred,y_predf], 1)\n",
    "method_list = ['Observations Raw',\n",
    "               'Observations Filtered',\n",
    "               'Predicted using raw data',\n",
    "               'Predicted using Filtered data']\n",
    "styles = {\n",
    "    'X':{'Preprocessing':'Raw','Data':'Training', 'Size':2},\n",
    "    'Xf':{'Preprocessing':'Filtered','Data':'Training', 'Size':2},\n",
    "    'Observations Raw':{'Preprocessing':'Raw','Data':'Training', 'Size':2},\n",
    "    'Observations Filtered':{'Preprocessing':'Filtered','Data':'Training', 'Size':2},\n",
    "    'Predicted using raw data':{'Preprocessing':'Raw','Data':'Predicted using raw data', 'Size':4},\n",
    "    'Predicted using Filtered data':{'Preprocessing':'Filtered','Data':'Predicted using Filtered data', 'Size':3},\n",
    "    \n",
    "}\n",
    "x_data, y_data = get_data(filtered=False, normalize=False)\n",
    "# region_mask = (np.mean(x_data,0) > 50).astype('int32')\n",
    "region_mask = (np.arange(n_regions) == 4).astype('int32')\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_prediction(x_test,x_testf, Ys, method_list, styles, region_names, region_mask)\n",
    "\n",
    "plt.savefig(f\"images/{DATASET}_{TRAINING_DATA_TYPE}.eps\")\n",
    "plt.savefig(f\"images/{DATASET}_{TRAINING_DATA_TYPE}.jpg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_data(X,Y,window=7):\n",
    "    '''\n",
    "    The dataset length will be reduced to guarante all samples have the window, so new length will be len(dataset)-window\n",
    "    '''\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for i in range(window-1,len(X)):\n",
    "        x.append(X[i-window+1:i+1]) \n",
    "        y.append(Y[i])\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "def get_model_predictions(model,x_data,y_data, scalers):\n",
    "    print(f\"Predicting from model. X={x_data.shape} Y={y_data.shape}\")\n",
    "    X_w,y_w = window_data(x_data,y_data,window=WINDOW_LENGTH)\n",
    "    \n",
    "    X_test_w = X_w[split_days-WINDOW_LENGTH-1:-1]\n",
    "    y_test_w = y_w[split_days-WINDOW_LENGTH-1:-1]\n",
    "    \n",
    "    if model.input.shape[-1]==1:\n",
    "        yhat = []\n",
    "        for col in range(n_regions):\n",
    "            yhat.append(model.predict(X_test_w[:,:,col:col+1])[:,0].reshape(1,-1)[0])\n",
    "        yhat = np.squeeze(np.array(yhat)).T\n",
    "    else:\n",
    "        yhat = model.predict(X_test_w[:,:,:])[:,0].reshape(-1,n_regions)\n",
    "        \n",
    "   \n",
    "    yhat = undo_normalization(yhat, scalers)[0]\n",
    "    y_test_w = undo_normalization(y_test_w, scalers)[0]\n",
    "    return X_test_w, y_test_w, yhat\n",
    "\n",
    "x_data, y_data, _ = get_data(filtered=False, normalize=x_data_scalers)\n",
    "_, y_test, yhat = get_model_predictions(model,x_data,y_data, x_data_scalers)\n",
    "x_dataf, y_dataf, _ = get_data(filtered=True, normalize=x_data_scalers)\n",
    "_, y_test, yhatf = get_model_predictions(model,x_dataf,y_dataf, x_data_scalers)\n",
    "\n",
    "x_data, y_data = get_data(filtered=False, normalize=False)\n",
    "x_dataf, y_dataf = get_data(filtered=True, normalize=False)\n",
    "# X = np.expand_dims(x_data[split_days-WINDOW_LENGTH:split_days,:],0)\n",
    "# Xf = np.expand_dims(x_dataf[split_days-WINDOW_LENGTH:split_days,:],0)\n",
    "X = np.expand_dims(x_data[:split_days,:],0)\n",
    "Xf = np.expand_dims(x_dataf[:split_days,:],0)\n",
    "Y = y_data[split_days-1:,:]\n",
    "Yf = y_dataf[split_days-1:,:]\n",
    "\n",
    "Ys = [Y, Yf, yhat, yhatf]\n",
    "method_list = ['Observations Raw',\n",
    "               'Observations Filtered',\n",
    "               f'Predictions using Raw data (Model {TRAINING_DATA_TYPE} {DATASET} data)',\n",
    "               f'Predictions using Filtered data (Model {TRAINING_DATA_TYPE} {DATASET} data)',\n",
    "               ]\n",
    "styles = {\n",
    "    'X':{'Preprocessing':'Raw','Data':'Training', 'Size':2},\n",
    "    'Xf':{'Preprocessing':'Filtered','Data':'Training', 'Size':2},\n",
    "    'Observations Raw':{'Preprocessing':'Raw','Data':'Training', 'Size':2},\n",
    "    'Observations Filtered':{'Preprocessing':'Filtered','Data':'Training', 'Size':2},\n",
    "    f'Predictions using Raw data (Model {TRAINING_DATA_TYPE} {DATASET} data)':{'Preprocessing':'Raw','Data':f'Predictions using Raw data (Model {TRAINING_DATA_TYPE} {DATASET} data)', 'Size':4},\n",
    "    f'Predictions using Filtered data (Model {TRAINING_DATA_TYPE} {DATASET} data)':{'Preprocessing':'Filtered','Data':f'Predictions using Filtered data (Model {TRAINING_DATA_TYPE} {DATASET} data)', 'Size':3},\n",
    "    \n",
    "}\n",
    "for i in range(len(Ys)):\n",
    "    print(method_list[i],Ys[i].shape)\n",
    "    Ys[i] = np.expand_dims(Ys[i],0)\n",
    "Ys = np.stack(Ys, 1)\n",
    "\n",
    "region_mask = ((200 > np.mean(x_data,0)) * (np.mean(x_data,0) > 00)).astype('int32')\n",
    "# region_mask = (np.arange(n_regions) == 6).astype('int32')\n",
    "\n",
    "plt.figure(figsize=(18,9))\n",
    "\n",
    "plot_prediction(X,Xf, Ys, method_list, styles, region_names, region_mask)\n",
    "\n",
    "plt.savefig(f\"images/{DATASET}_DayByDay.eps\")\n",
    "plt.savefig(f\"images/{DATASET}_DayByDay.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = WINDOW_LENGTH\n",
    "r = model.input.shape[-1]\n",
    "start_seqs = [np.random.random((1,x,r)),\n",
    "              np.ones((1,x,r))*0,\n",
    "              np.ones((1,x,r))*0.5,\n",
    "              np.ones((1,x,r))*1,\n",
    "              np.arange(x*r).reshape((1,x,r))/30,\n",
    "              np.sin(np.arange(x)/x*np.pi/2).reshape((1,x,1)).repeat(r,-1)\n",
    "]\n",
    "\n",
    "predictions = []\n",
    "for start_seq in start_seqs:\n",
    "    input_seq = np.copy(start_seq)\n",
    "    print(input_seq.shape)\n",
    "    predict_seq = [start_seq[0,:,:]]\n",
    "    for _ in range(50):\n",
    "        output = model(input_seq, training=False)\n",
    "        \n",
    "        input_seq = input_seq[:,output.shape[1]:,:]\n",
    "        if len(output.shape)==2:\n",
    "            output = np.expand_dims(output,-1)\n",
    "        predict_seq.append(output[0])\n",
    "        input_seq = np.concatenate([input_seq, output],1)\n",
    "    predictions.append(np.concatenate(predict_seq,0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(1+np.array(predictions)[:,:30,0].T)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
